<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/src/integrate_pipeline.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/integrate_pipeline.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Integración de Goodreads (JSON) y Google Books (CSV) a artefactos estándar.&#10;Produce standard/dim_book.parquet, standard/book_source_detail.parquet y docs/quality_metrics.json&#10;Mejoras implementadas:&#10;- Merge robusto por isbn13 y por match_key (titulo+autor) sin producir producto cartesiano&#10;- Aserciones configurables: unicidad book_id (bloqueante), porcentaje mínimo de títulos no nulos (por defecto 90%)&#10;- Fail-soft configurado: registros con errores se marcan en book_source_detail y se excluyen de dim_book&#10;- Resumen de aserciones incluido en docs/quality_metrics.json&#10;&quot;&quot;&quot;&#10;from __future__ import annotations&#10;&#10;import hashlib&#10;import json&#10;import logging&#10;from datetime import datetime, timezone&#10;from pathlib import Path&#10;from typing import Optional, Tuple, Dict, List, Mapping&#10;&#10;import pandas as pd&#10;import traceback&#10;import warnings&#10;from difflib import SequenceMatcher&#10;&#10;# Suprimir FutureWarnings ruidosos de pandas que no afectan la lógica actual&#10;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning, message=&quot;.*downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated.*&quot;)&#10;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning, message=&quot;.*DataFrameGroupBy.apply operated on the grouping columns.*&quot;)&#10;&#10;# is_valid_isbn13 se usa en otros módulos; no es necesario importarlo aquí para evitar warning&#10;# from utils_isbn import is_valid_isbn13&#10;from utils_quality import (&#10;    compute_quality_metrics,&#10;    listify,&#10;    normalize_language,&#10;    normalize_whitespace,&#10;    parse_date_to_iso,&#10;    uniq_preserve,&#10;    validate_currency,&#10;    validate_language,&#10;    normalize_currency, nulls_by_column,&#10;)&#10;# Añadir import de utils_isbn necesario&#10;from utils_isbn import try_normalize_isbn, is_valid_isbn13&#10;&#10;# Evitar FutureWarning sobre downcasting en operaciones futuras de pandas&#10;try:&#10;    pd.set_option('future.no_silent_downcasting', True)&#10;except Exception:&#10;    pass&#10;&#10;# Funciones auxiliares para sanitizar registros antes de calcular métricas&#10;def _sanitize_value(v):&#10;    try:&#10;        if v is None:&#10;            return None&#10;        if isinstance(v, float) and pd.isna(v):&#10;            return None&#10;        if isinstance(v, (bytes, bytearray)):&#10;            try:&#10;                return v.decode('utf-8')&#10;            except Exception:&#10;                return str(v)&#10;        if isinstance(v, (str, bool, int, float)):&#10;            return v&#10;        if isinstance(v, (list, dict)):&#10;            try:&#10;                return json.dumps(v, ensure_ascii=False)&#10;            except Exception:&#10;                return str(v)&#10;        if hasattr(v, 'tolist'):&#10;            try:&#10;                return v.tolist()&#10;            except Exception:&#10;                pass&#10;        return str(v)&#10;    except Exception:&#10;        return None&#10;&#10;&#10;def _sanitize_records_for_metrics(records: List[Mapping[str, object]]) -&gt; List[Mapping[str, object]]:&#10;    sanitized: List[Dict[str, object]] = []&#10;    for r in records:&#10;        if not isinstance(r, Mapping):&#10;            sanitized.append({})&#10;            continue&#10;        row: Dict[str, object] = {}&#10;        for k, v in r.items():&#10;            try:&#10;                row[k] = _sanitize_value(v)&#10;            except Exception:&#10;                row[k] = None&#10;        sanitized.append(row)&#10;    return sanitized&#10;&#10;&#10;# Helpers seguros para leer y escribir en DataFrames con índices no estándar&#10;def _safe_set(df: pd.DataFrame, idx, col: str, val) -&gt; bool:&#10;    try:&#10;        if idx in df.index:&#10;            df.loc[idx, col] = val&#10;            return True&#10;    except Exception:&#10;        pass&#10;    try:&#10;        pos = int(idx)&#10;        df.iloc[pos, df.columns.get_loc(col)] = val&#10;        return True&#10;    except Exception:&#10;        pass&#10;    try:&#10;        df.at[idx, col] = val&#10;        return True&#10;    except Exception:&#10;        return False&#10;&#10;&#10;def _safe_get(df: pd.DataFrame, idx, col: str):&#10;    try:&#10;        if idx in df.index:&#10;            return df.loc[idx, col]&#10;    except Exception:&#10;        pass&#10;    try:&#10;        pos = int(idx)&#10;        return df.iloc[pos][col]&#10;    except Exception:&#10;        pass&#10;    try:&#10;        return df.at[idx, col]&#10;    except Exception:&#10;        return None&#10;&#10;# Import robusto de funciones de logging en work/&#10;try:&#10;    from work.utils_logging import log_rule_jsonl, write_run_summary, ensure_work_dirs&#10;except Exception:&#10;    def log_rule_jsonl(*args, **kwargs):&#10;        return None&#10;    def write_run_summary(*args, **kwargs):&#10;        return None&#10;    def ensure_work_dirs(*args, **kwargs):&#10;        return None&#10;&#10;# Configuración y paths&#10;ROOT = Path(__file__).resolve().parents[1]&#10;LANDING = ROOT / &quot;landing&quot;&#10;STANDARD = ROOT / &quot;standard&quot;&#10;DOCS = ROOT / &quot;docs&quot;&#10;&#10;# Aserciones / umbrales para la rúbrica&#10;ASSERT_UNIQUENESS_BOOK_ID = True  # si True, bloquear si existen duplicados irreconciliables&#10;MIN_TITLES_PCT = 0.90  # mínimo % de títulos no nulos requerido&#10;# Umbrales para matching con candidatos de Google Books&#10;# umbral reducido para aceptar candidatos más amplios y mejorar cobertura de ISBN/idioma/fecha&#10;GB_MATCH_THRESHOLD = 20.0  # puntuación mínima para aceptar candidato de Google Books cuando Goodreads no tiene ISBN&#10;# Umbral para matching difuso entre títulos/autores (0..1). Ajustable.&#10;GB_FUZZY_THRESHOLD = 0.75&#10;&#10;# Logger sencillo&#10;logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')&#10;logger = logging.getLogger(__name__)&#10;&#10;# Mapeo sencillo de símbolos a ISO para monedas&#10;SYMBOL_TO_ISO = {&#10;    &quot;$&quot;: &quot;USD&quot;,&#10;    &quot;\u20ac&quot;: &quot;EUR&quot;,&#10;    &quot;\u00a3&quot;: &quot;GBP&quot;,&#10;    &quot;\u00a5&quot;: &quot;JPY&quot;,&#10;    &quot;R$&quot;: &quot;BRL&quot;,&#10;}&#10;&#10;&#10;def _canonical_key(title: str, author: str, publisher: Optional[str], year: Optional[int]) -&gt; str:&#10;    # Genera clave SHA1 estable para fallback cuando isbn13 no existe&#10;    def _norm(x: Optional[object]) -&gt; str:&#10;        if x is None or (isinstance(x, float) and pd.isna(x)):&#10;            return &quot;&quot;&#10;        s = str(x)&#10;        return s.strip().lower()&#10;&#10;    base = &quot;|&quot;.join([&#10;        _norm(title),&#10;        _norm(author),&#10;        _norm(publisher),&#10;        _norm(year),&#10;    ])&#10;    return hashlib.sha1(base.encode(&quot;utf-8&quot;)).hexdigest()&#10;&#10;&#10;def _load_sources() -&gt; Tuple[pd.DataFrame, pd.DataFrame]:&#10;    &quot;&quot;&quot;Carga archivos de landing asegurando ISBN como string para evitar coerciones a int.&#10;&#10;    Comentarios:&#10;    - Supuesto: los ficheros en `landing/` son la &quot;fuente de la verdad&quot; y no deben ser sobrescritos por integración.&#10;    - Se fuerza dtype string para isbn para evitar conversión automática a int/float que rompe pyarrow.&#10;    KEYWORDS: LOAD_SOURCES, COERCE_ISBN&#10;    &quot;&quot;&quot;&#10;    gr_path = LANDING / &quot;goodreads_books.json&quot;&#10;    gb_path = LANDING / &quot;googlebooks_books.csv&quot;&#10;&#10;    if not gr_path.exists():&#10;        raise FileNotFoundError(f&quot;Falta archivo de landing esperado: {gr_path}&quot;)&#10;    if not gb_path.exists():&#10;        raise FileNotFoundError(f&quot;Falta archivo de landing esperado: {gb_path}&quot;)&#10;&#10;    with open(gr_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        gr_json = json.load(f)&#10;    gr = pd.DataFrame(gr_json.get(&quot;records&quot;, []))&#10;    gr = gr.copy()&#10;    gr[&quot;source_name&quot;] = &quot;goodreads&quot;&#10;    gr[&quot;source_file&quot;] = str(gr_path.name)&#10;&#10;    gb = pd.read_csv(gb_path, dtype={&quot;isbn13&quot;: &quot;string&quot;, &quot;isbn10&quot;: &quot;string&quot;})&#10;    gb = gb.copy()&#10;    gb[&quot;source_name&quot;] = &quot;google_books&quot;&#10;    gb[&quot;source_file&quot;] = str(gb_path.name)&#10;&#10;    # --- nuevo: exponer numero de fila CSV para poder mapear con googlebooks_candidates.csv ---&#10;    try:&#10;        import numpy as _np&#10;        gb['_csv_row'] = _np.arange(1, len(gb) + 1)&#10;    except:&#10;        gb['_csv_row'] = list(range(1, len(gb) + 1))&#10;&#10;    # Nota: anteriormente aquí se intentaba usar `gb_price_map` que no existe en este&#10;    # contexto (provoca NameError). La lógica de mapeo de precios se realiza más abajo&#10;    # dentro de `_merge_sources` a partir de `googlebooks_candidates.json` y, por tanto,&#10;    # no debe ejecutarse ni referenciarse aquí.&#10;&#10;    return gr, gb&#10;&#10;&#10;def _normalize_frames(gr: pd.DataFrame, gb: pd.DataFrame) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:&#10;    # Normaliza columnas y formatos básicos en ambos dataframes&#10;    gr = gr.copy()&#10;    gr.rename(columns={&#10;        &quot;title&quot;: &quot;titulo&quot;,&#10;        &quot;author&quot;: &quot;autor_principal&quot;,&#10;        &quot;isbn13&quot;: &quot;isbn13&quot;,&#10;        &quot;isbn10&quot;: &quot;isbn10&quot;,&#10;    }, inplace=True)&#10;    for c in (&quot;isbn13&quot;, &quot;isbn10&quot;):&#10;        if c in gr.columns:&#10;            gr[c] = gr[c].apply(lambda v: str(v).strip() if pd.notna(v) and str(v).strip() != &quot;&quot; else None)&#10;    gr[&quot;idioma&quot;] = None&#10;    gr[&quot;categoria&quot;] = None&#10;    gr[&quot;fecha_publicacion&quot;] = None&#10;    gr[&quot;precio&quot;] = None&#10;    gr[&quot;moneda&quot;] = None&#10;&#10;    gb = gb.copy()&#10;    gb.rename(columns={&#10;        &quot;title&quot;: &quot;titulo&quot;,&#10;        &quot;authors&quot;: &quot;autores&quot;,&#10;        &quot;publisher&quot;: &quot;editorial&quot;,&#10;        &quot;pub_date&quot;: &quot;fecha_publicacion&quot;,&#10;        &quot;language&quot;: &quot;idioma&quot;,&#10;        &quot;categories&quot;: &quot;categoria&quot;,&#10;        &quot;price_amount&quot;: &quot;precio&quot;,&#10;        &quot;price_currency&quot;: &quot;moneda&quot;,&#10;    }, inplace=True)&#10;    for c in (&quot;isbn13&quot;, &quot;isbn10&quot;):&#10;        if c in gb.columns:&#10;            gb[c] = gb[c].apply(lambda v: str(v).strip() if pd.notna(v) and str(v).strip() != &quot;&quot; else None)&#10;&#10;    # autores/categoria como listas serializadas&#10;    if &quot;autores&quot; in gb.columns:&#10;        gb[&quot;autores&quot;] = gb[&quot;autores&quot;].apply(lambda x: &quot;;&quot;.join(uniq_preserve(listify(x))) if pd.notna(x) else None)&#10;        gb[&quot;autor_principal&quot;] = gb[&quot;autores&quot;].apply(lambda x: str(x).split(&quot;;&quot;)[0] if pd.notna(x) and str(x).strip() != &quot;&quot; else None)&#10;    else:&#10;        gb[&quot;autor_principal&quot;] = None&#10;    if &quot;categoria&quot; in gb.columns:&#10;        gb[&quot;categoria&quot;] = gb[&quot;categoria&quot;].apply(lambda x: &quot;;&quot;.join(uniq_preserve(listify(x))) if pd.notna(x) else None)&#10;&#10;    if &quot;idioma&quot; in gb.columns:&#10;        gb[&quot;idioma&quot;] = gb[&quot;idioma&quot;].apply(lambda x: normalize_language(x) if pd.notna(x) else None)&#10;&#10;    gb[&quot;_match_title&quot;] = gb[&quot;titulo&quot;].apply(lambda s: normalize_whitespace(str(s).lower()) if pd.notna(s) else None)&#10;    gb[&quot;_match_author&quot;] = gb[&quot;autor_principal&quot;].apply(lambda s: normalize_whitespace(str(s).lower()) if pd.notna(s) else None)&#10;&#10;    # Para Goodreads crear keys similares para emparejar&#10;    gr[&quot;_match_title&quot;] = gr.apply(lambda r: normalize_whitespace(str(r.get(&quot;titulo&quot;) or &quot;&quot;).lower()), axis=1)&#10;    gr[&quot;_match_author&quot;] = gr.apply(lambda r: normalize_whitespace(str(r.get(&quot;autor_principal&quot;) or &quot;&quot;).lower()), axis=1)&#10;    gr[&quot;_match_key&quot;] = gr.apply(lambda r: (f&quot;{r.get('_match_title')}|{r.get('_match_author')}&quot; if r.get('_match_title') or r.get('_match_author') else None), axis=1)&#10;    gb[&quot;_match_key&quot;] = gb.apply(lambda r: (f&quot;{r.get('_match_title') or ''}|{r.get('_match_author') or ''}&quot; if (r.get('_match_title') or r.get('_match_author')) else None), axis=1)&#10;&#10;    return gr, gb&#10;&#10;&#10;def _merge_sources(gr: pd.DataFrame, gb: pd.DataFrame) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;Merge robusto por isbn13 (primario) y por _match_key (secundario) sin producir producto cartesiano.&#10;    Devuelve DataFrame con columnas limpias y sufijos explícitos para selección posterior.&#10;    &quot;&quot;&quot;&#10;    # Intentar asignar ISBN a registros de Goodreads sin isbn usando candidatos de googlebooks_candidates.json&#10;    candidates_path = LANDING / &quot;googlebooks_candidates.json&quot;&#10;    gb_cands_map: Dict[str, Tuple[Optional[str], Optional[float]]] = {}&#10;    gb_price_map: Dict[str, Tuple[Optional[float], Optional[str]]] = {}&#10;    if candidates_path.exists():&#10;        try:&#10;            with open(candidates_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;                cand_list = json.load(f)&#10;            for c in cand_list:&#10;                rec_id = c.get(&quot;rec_id&quot;)&#10;                idx = c.get(&quot;input_index&quot;)&#10;                best_score = c.get(&quot;best_score&quot;)&#10;                chosen_isbn = None&#10;                # elegir el candidate con mayor score que tenga isbn13&#10;                for cand in (c.get(&quot;candidates&quot;) or []):&#10;                    if cand.get(&quot;isbn13&quot;) and (best_score is None or float(cand.get(&quot;score&quot;, 0)) &gt;= float(best_score) - 0.0001):&#10;                        chosen_isbn = cand.get(&quot;isbn13&quot;)&#10;                        break&#10;                # fallback: tomar el candidato con mayor score que tenga isbn13&#10;                if not chosen_isbn:&#10;                    best_c = None&#10;                    best_s = -1.0&#10;                    for cand in (c.get(&quot;candidates&quot;) or []):&#10;                        s = float(cand.get(&quot;score&quot;)) if cand.get(&quot;score&quot;) is not None else 0.0&#10;                        if cand.get(&quot;isbn13&quot;) and s &gt; best_s:&#10;                            best_s = s&#10;                            best_c = cand&#10;                    if best_c:&#10;                        chosen_isbn = best_c.get(&quot;isbn13&quot;)&#10;                        best_score = best_s&#10;                if rec_id:&#10;                    gb_cands_map[f&quot;rec:{rec_id}&quot;] = (chosen_isbn, best_score)&#10;                if idx is not None:&#10;                    gb_cands_map[f&quot;idx:{idx}&quot;] = (chosen_isbn, best_score)&#10;                # title-author key&#10;                t = c.get(&quot;title&quot;) or &quot;&quot;&#10;                a = c.get(&quot;author&quot;) or &quot;&quot;&#10;                gb_cands_map[f&quot;ta:{t.strip()}|{a.strip()}&quot;] = (chosen_isbn, best_score)&#10;&#10;                # --- nuevo: mapear precio/moneda desde candidatos para mejorar cobertura ---&#10;                # buscar any candidate with currency&#10;                price_amt = None&#10;                price_cur = None&#10;                # iterar TODOS los candidatos y tomar el primero con currency si existe&#10;                for cand in (c.get('candidates') or []):&#10;                    if cand.get('price_currency') and cand.get('price_currency') not in (None, '', pd.NA):&#10;                        price_amt = cand.get('price_amount')&#10;                        price_cur = cand.get('price_currency')&#10;                        break&#10;                # asignar a varias keys (rec, idx y ta y row:{csv_row_number}) para lookup posterior&#10;                if rec_id:&#10;                    gb_price_map[f&quot;rec:{rec_id}&quot;] = (price_amt, price_cur)&#10;                if idx is not None:&#10;                    gb_price_map[f&quot;idx:{idx}&quot;] = (price_amt, price_cur)&#10;                csv_row = c.get('csv_row_number')&#10;                if csv_row is not None:&#10;                    gb_price_map[f&quot;row:{csv_row}&quot;] = (price_amt, price_cur)&#10;                # crear clave normalizada para title|author&#10;                t_key = normalize_whitespace(t).lower() if t else ''&#10;                a_key = normalize_whitespace(a).lower() if a else ''&#10;                gb_price_map[f&quot;ta:{t_key}|{a_key}&quot;] = (price_amt, price_cur)&#10;        except Exception:&#10;            gb_cands_map = {}&#10;            gb_price_map = {}&#10;&#10;    # Aplicar precios/monedas detectados en gb_price_map a filas de 'gb' cuando sea posible.&#10;    # Soporta claves 'row:' y claves normalizadas 'ta:' usando gb._match_key&#10;    try:&#10;        if gb_price_map and isinstance(gb, pd.DataFrame):&#10;            for key, (pamt, pcur) in gb_price_map.items():&#10;                if key.startswith('row:'):&#10;                    try:&#10;                        part = key.split(':', 1)[1]&#10;                        rownum = int(part) if (part is not None and str(part).isdigit()) else None&#10;                        mask = gb.get('_csv_row') == rownum&#10;                        if 'precio' in gb.columns and pamt is not None:&#10;                            gb.loc[mask &amp; gb['precio'].isna(), 'precio'] = pamt&#10;                        if 'moneda' in gb.columns and pcur is not None:&#10;                            gb.loc[mask &amp; gb['moneda'].isna(), 'moneda'] = pcur&#10;                    except Exception:&#10;                        continue&#10;                elif key.startswith('ta:'):&#10;                    try:&#10;                        # key form: ta:normalized_title|normalized_author&#10;                        _, ta = key.split(':', 1)&#10;                        tpart, apart = ta.split('|', 1)&#10;                        # usar _match_key creado en _normalize_frames para comparar&#10;                        match_key = f&quot;{tpart}|{apart}&quot;&#10;                        mask = gb.get('_match_key') == match_key&#10;                        if 'precio' in gb.columns and pamt is not None:&#10;                            gb.loc[mask &amp; gb['precio'].isna(), 'precio'] = pamt&#10;                        if 'moneda' in gb.columns and pcur is not None:&#10;                            gb.loc[mask &amp; gb['moneda'].isna(), 'moneda'] = pcur&#10;                    except Exception:&#10;                        continue&#10;    except Exception:&#10;        # no bloquear si no se puede aplicar el mapeo&#10;        pass&#10;&#10;    # Dividir gr en con isbn y sin isbn&#10;    gr_with_isbn = gr[gr[&quot;isbn13&quot;].notna()].copy()&#10;    gr_no_isbn = gr[gr[&quot;isbn13&quot;].isna()].copy()&#10;&#10;    # Asignar isbn13 desde candidatos cuando el score supera umbral&#10;    def _assign_isbn_from_candidates(row):&#10;        # buscar por rec_id (book_url), index o title|author&#10;        rec = row.get(&quot;book_url&quot;) or row.get(&quot;id&quot;)&#10;        keys = []&#10;        if rec:&#10;            keys.append(f&quot;rec:{rec}&quot;)&#10;        # index-based key (input index)&#10;        if row.name is not None:&#10;            try:&#10;                safe_idx = None&#10;                try:&#10;                    if isinstance(row.name, (int, float)):&#10;                        safe_idx = int(row.name)&#10;                    else:&#10;                        rs = str(row.name).strip()&#10;                        if rs.isdigit():&#10;                            safe_idx = int(rs)&#10;                except Exception:&#10;                    safe_idx = None&#10;                if safe_idx is not None:&#10;                    keys.append(f&quot;idx:{safe_idx}&quot;)&#10;            except Exception:&#10;                pass&#10;        # intentar varias formas de title|author: original fields y campos normalizados&#10;        ta_candidates = []&#10;        ta_candidates.append(f&quot;ta:{(row.get('title') or '').strip()}|{(row.get('author') or '').strip()}&quot;)&#10;        ta_candidates.append(f&quot;ta:{(row.get('titulo') or row.get('title') or '').strip()}|{(row.get('autor_principal') or row.get('author') or '').strip()}&quot;)&#10;        # también intentar usar _match_key si está presente (normalizado título|autor)&#10;        mk = None&#10;        if '_match_key' in row.index:&#10;            mk = row.get('_match_key')&#10;        elif row.get('_match_title') is not None or row.get('_match_author') is not None:&#10;            mk = f&quot;{row.get('_match_title') or ''}|{row.get('_match_author') or ''}&quot;&#10;        if mk:&#10;            keys.append(f&quot;ta:{mk}&quot;)&#10;        # añadir ta candidates&#10;        keys.extend(ta_candidates)&#10;&#10;        for k in keys:&#10;            if not k:&#10;                continue&#10;            if k in gb_cands_map:&#10;                isbn, score = gb_cands_map[k]&#10;                try:&#10;                    if isbn and score is not None and float(score) &gt;= GB_MATCH_THRESHOLD:&#10;                        # Registrar regla: asignación de ISBN desde candidatos&#10;                        try:&#10;                            log_rule_jsonl({&#10;                                'rule': 'assign_isbn_from_candidates',&#10;                                'key': k,&#10;                                'assigned_isbn': isbn,&#10;                                'score': float(score),&#10;                                'rec': rec,&#10;                            })&#10;                        except Exception:&#10;                            pass&#10;                        return isbn, float(score)&#10;                except Exception:&#10;                    continue&#10;        return None, None&#10;&#10;    if not gr_no_isbn.empty and gb_cands_map:&#10;        assigned = gr_no_isbn.apply(lambda r: _assign_isbn_from_candidates(r), axis=1, result_type='expand')&#10;        # manejar distintos tipos de retorno de apply: DataFrame, Series o ndarray&#10;        try:&#10;            if isinstance(assigned, pd.DataFrame) and assigned.shape[1] &gt;= 1:&#10;                gr_no_isbn['isbn13_assigned'] = assigned.iloc[:, 0]&#10;                if assigned.shape[1] &gt; 1:&#10;                    gr_no_isbn['gb_assigned_score'] = assigned.iloc[:, 1]&#10;            elif isinstance(assigned, pd.Series):&#10;                # cada elemento puede ser tuple/list -&gt; expandir con seguridad&#10;                gr_no_isbn['isbn13_assigned'] = assigned.apply(lambda v: v[0] if isinstance(v, (list, tuple)) and len(v) &gt; 0 else None)&#10;                gr_no_isbn['gb_assigned_score'] = assigned.apply(lambda v: v[1] if isinstance(v, (list, tuple)) and len(v) &gt; 1 else None)&#10;            else:&#10;                # intentar convertir a array y extraer columnas si posible&#10;                try:&#10;                    arr = pd.DataFrame(list(assigned))&#10;                    if arr.shape[1] &gt;= 1:&#10;                        gr_no_isbn['isbn13_assigned'] = arr.iloc[:, 0].values&#10;                    if arr.shape[1] &gt; 1:&#10;                        gr_no_isbn['gb_assigned_score'] = arr.iloc[:, 1].values&#10;                except Exception:&#10;                    pass&#10;        except Exception:&#10;            # no bloquear pipeline por fallo al interpretar 'assigned'&#10;            pass&#10;        # promover asignación a isbn13 si existe&#10;        try:&#10;            if 'isbn13_assigned' in gr_no_isbn.columns:&#10;                gr_no_isbn.loc[gr_no_isbn['isbn13_assigned'].notna(), 'isbn13'] = gr_no_isbn.loc[gr_no_isbn['isbn13_assigned'].notna(), 'isbn13_assigned']&#10;                gr_no_isbn.drop(columns=['isbn13_assigned'], inplace=True, errors='ignore')&#10;        except Exception:&#10;            pass&#10;&#10;    # --- Fallback difuso: si tras candidates aún hay registros sin isbn, hacer fuzzy match contra GB ---&#10;    try:&#10;        # preparar comparación: usar _match_title/_match_author normalizados en gb&#10;        if not gr_no_isbn.empty and isinstance(gb, pd.DataFrame):&#10;            # garantizar columnas de normalización en gb&#10;            if '_match_title' not in gb.columns:&#10;                gb['_match_title'] = gb['titulo'].apply(lambda s: normalize_whitespace(str(s).lower()) if pd.notna(s) else '')&#10;            if '_match_author' not in gb.columns:&#10;                gb['_match_author'] = gb['autor_principal'].apply(lambda s: normalize_whitespace(str(s).lower()) if pd.notna(s) else '')&#10;&#10;            def _score_match(gr_title, gr_author, gb_title, gb_author, gb_isbn_present):&#10;                # title and author are normalized strings&#10;                try:&#10;                    title_ratio = SequenceMatcher(None, gr_title, gb_title).ratio() if gr_title and gb_title else 0.0&#10;                except Exception:&#10;                    title_ratio = 0.0&#10;                try:&#10;                    author_ratio = SequenceMatcher(None, gr_author, gb_author).ratio() if gr_author and gb_author else 0.0&#10;                except Exception:&#10;                    author_ratio = 0.0&#10;                # token overlap on titles&#10;                try:&#10;                    tset_g = set([w for w in gr_title.split() if w])&#10;                    tset_b = set([w for w in gb_title.split() if w])&#10;                    token_overlap = (len(tset_g &amp; tset_b) / max(len(tset_g | tset_b), 1)) if tset_g or tset_b else 0.0&#10;                except Exception:&#10;                    token_overlap = 0.0&#10;                score = 0.6 * title_ratio + 0.3 * author_ratio + 0.1 * token_overlap&#10;                # dar leve bonus si GB row tiene isbn&#10;                if gb_isbn_present:&#10;                    score += 0.05&#10;                return float(score)&#10;&#10;            fuzzy_results = []&#10;            for idx, gr_row in gr_no_isbn.iterrows():&#10;                if pd.notna(gr_row.get('isbn13')) and str(gr_row.get('isbn13')).strip() != '':&#10;                    continue&#10;                gr_title_norm = normalize_whitespace(str(gr_row.get('titulo') or gr_row.get('title') or '').lower())&#10;                gr_author_norm = normalize_whitespace(str(gr_row.get('autor_principal') or gr_row.get('author') or '').lower())&#10;                best_score = -1.0&#10;                best_isbn = None&#10;                best_gb_index = None&#10;                # iterar gb y puntuar&#10;                for gb_i, gb_row in gb.iterrows():&#10;                    gb_title = str(gb_row.get('_match_title') or '').lower()&#10;                    gb_author = str(gb_row.get('_match_author') or '').lower()&#10;                    gb_has_isbn = bool(gb_row.get('isbn13'))&#10;                    s = _score_match(gr_title_norm, gr_author_norm, gb_title, gb_author, gb_has_isbn)&#10;                    if s &gt; best_score:&#10;                        best_score = s&#10;                        best_isbn = gb_row.get('isbn13')&#10;                        best_gb_index = gb_i&#10;                # si mejor score supera umbral, asignar&#10;                try:&#10;                    if best_score &gt;= GB_FUZZY_THRESHOLD and best_isbn:&#10;                        # registrar regla&#10;                        try:&#10;                            gi = int(idx) if isinstance(idx, (int, float)) else (int(str(idx)) if isinstance(idx, str) and str(idx).isdigit() else str(idx))&#10;                            gbi = int(best_gb_index) if (best_gb_index is not None and (isinstance(best_gb_index, (int, float)) or (isinstance(best_gb_index, str) and str(best_gb_index).isdigit()))) else best_gb_index&#10;                            log_rule_jsonl({'rule': 'fuzzy_assign_isbn', 'gr_index': gi, 'assigned_isbn': best_isbn, 'score': float(best_score), 'gb_index': gbi})&#10;                        except Exception:&#10;                            pass&#10;                        _safe_set(gr_no_isbn, idx, 'isbn13_assigned', best_isbn)&#10;                        _safe_set(gr_no_isbn, idx, 'gb_assigned_score', float(best_score))&#10;                except Exception:&#10;                    continue&#10;    except Exception:&#10;        # no bloquear pipeline si fuzzy fallback falla&#10;        pass&#10;&#10;    logger.info(&quot;gr_total=%d gr_with_isbn=%d gr_no_isbn=%d gb_total=%d&quot;, len(gr), len(gr_with_isbn), len(gr_no_isbn), len(gb))&#10;&#10;    # Merge por isbn13 para todos los registros que tienen isbn13 en gr&#10;    merged_by_isbn = pd.merge(&#10;        gr_with_isbn.add_suffix(&quot;_gr&quot;),&#10;        gb.add_suffix(&quot;_gb&quot;),&#10;        left_on=&quot;isbn13_gr&quot;,&#10;        right_on=&quot;isbn13_gb&quot;,&#10;        how=&quot;left&quot;,&#10;        suffixes=(&quot;_gr&quot;, &quot;_gb&quot;),&#10;    )&#10;&#10;    # Merge por match_key para registros sin isbn (título+autor normalizados)&#10;    merged_by_key = pd.merge(&#10;        gr_no_isbn.add_suffix(&quot;_gr&quot;),&#10;        gb.add_suffix(&quot;_gb&quot;),&#10;        left_on=&quot;_match_key_gr&quot;,&#10;        right_on=&quot;_match_key_gb&quot;,&#10;        how=&quot;left&quot;,&#10;        suffixes=(&quot;_gr&quot;, &quot;_gb&quot;),&#10;    )&#10;&#10;    # Identificar gb rows que no han sido emparejadas por isbn ni por key (gb-only)&#10;    # Mejor detección de filas de GB ya emparejadas: usar _csv_row si está disponible&#10;    matched_gb_rows = set()&#10;    matched_gb_isbns = set()&#10;    try:&#10;        if '_csv_row_gb' in merged_by_isbn.columns:&#10;            matched_gb_rows.update(merged_by_isbn['_csv_row_gb'].dropna().astype(int).astype(str).unique().tolist())&#10;        if '_csv_row_gb' in merged_by_key.columns:&#10;            matched_gb_rows.update(merged_by_key['_csv_row_gb'].dropna().astype(int).astype(str).unique().tolist())&#10;    except Exception:&#10;        # fallback a ISBN si no podemos usar _csv_row&#10;        try:&#10;            matched_gb_isbns.update(merged_by_isbn['isbn13_gb'].dropna().unique().tolist())&#10;            matched_gb_isbns.update(merged_by_key['isbn13_gb'].dropna().unique().tolist())&#10;        except Exception:&#10;            matched_gb_isbns = set()&#10;&#10;    if matched_gb_rows:&#10;        # gb['_csv_row'] fue creado en _load_sources; comparar como strings/números&#10;        try:&#10;            gb_only_mask = ~gb['_csv_row'].astype(str).isin(matched_gb_rows)&#10;        except Exception:&#10;            gb_only_mask = ~gb['_csv_row'].isin([int(x) for x in matched_gb_rows if str(x).isdigit()])&#10;    else:&#10;        # uso de ISBN como fallback&#10;        try:&#10;            gb_only_mask = ~gb['isbn13'].isin([v for v in matched_gb_isbns if v is not None])&#10;        except Exception:&#10;            gb_only_mask = pd.Series([True] * len(gb), index=gb.index)&#10;&#10;    gb_only = gb[gb_only_mask].copy()&#10;&#10;    # --- diagnóstico agregado de matches ---&#10;    try:&#10;        logger.info('gb rows total=%d; gb_only after filtering=%d; gr_with_isbn=%d; gr_no_isbn=%d', len(gb), len(gb_only), len(gr_with_isbn), len(gr_no_isbn))&#10;    except Exception:&#10;        pass&#10;&#10;    # Normalizar estructuras para concatenar: queremos columnas con sufijos *_gr y *_gb&#10;    def _ensure_columns(df: pd.DataFrame, cols: List[str]) -&gt; pd.DataFrame:&#10;        for c in cols:&#10;            if c not in df.columns:&#10;                df[c] = None&#10;        return df&#10;&#10;    # Determinar conjunto de columnas esperadas (sufijos)&#10;    cols_gr = [c for c in merged_by_isbn.columns if c.endswith(&quot;_gr&quot;)] + [c for c in merged_by_key.columns if c.endswith(&quot;_gr&quot;)]&#10;    cols_gb = [c for c in merged_by_isbn.columns if c.endswith(&quot;_gb&quot;)] + [c for c in merged_by_key.columns if c.endswith(&quot;_gb&quot;)]&#10;    cols_gr = list(dict.fromkeys(cols_gr))&#10;    cols_gb = list(dict.fromkeys(cols_gb))&#10;&#10;    # Preparar gb_only en formato con sufijo _gb&#10;    gb_only_prefixed = gb_only.copy()&#10;    gb_only_prefixed = gb_only_prefixed.add_suffix(&quot;_gb&quot;)&#10;    # marcar origen para facilitar filtrado posterior: gb_only&#10;    try:&#10;        gb_only_prefixed['_source_type'] = 'gb_only'&#10;    except Exception:&#10;        pass&#10;    # Añadir columnas _gr vacías para compatibilidad&#10;    for c in cols_gr:&#10;        if c not in gb_only_prefixed.columns:&#10;            gb_only_prefixed[c] = None&#10;    # Asegurar columnas esperadas en merged_by_isbn y merged_by_key&#10;    merged_by_isbn = _ensure_columns(merged_by_isbn, cols_gb + cols_gr)&#10;    merged_by_key = _ensure_columns(merged_by_key, cols_gb + cols_gr)&#10;    # marcar origen para merges basados en isbn/key&#10;    try:&#10;        merged_by_isbn['_source_type'] = 'merged_by_isbn'&#10;    except Exception:&#10;        pass&#10;    try:&#10;        merged_by_key['_source_type'] = 'merged_by_key'&#10;    except Exception:&#10;        pass&#10;&#10;    # Concatenar todas las filas en un único merged (alineado por columnas)&#10;    # Evitar FutureWarning: eliminar columnas que sean totalmente NA en cada DataFrame&#10;    def _drop_all_na_cols(df: pd.DataFrame) -&gt; pd.DataFrame:&#10;        try:&#10;            return df.dropna(axis=1, how='all')&#10;        except Exception:&#10;            return df&#10;&#10;    merged_by_isbn = _drop_all_na_cols(merged_by_isbn)&#10;    merged_by_key = _drop_all_na_cols(merged_by_key)&#10;    gb_only_prefixed = _drop_all_na_cols(gb_only_prefixed)&#10;&#10;    # Log de diagnóstico: tamaños intermedios antes de concatenar&#10;    try:&#10;        logger.info('Merged components sizes: by_isbn=%d by_key=%d gb_only=%d',&#10;                    0 if merged_by_isbn is None else (len(merged_by_isbn) if hasattr(merged_by_isbn, '__len__') else 0),&#10;                    0 if merged_by_key is None else (len(merged_by_key) if hasattr(merged_by_key, '__len__') else 0),&#10;                    0 if gb_only_prefixed is None else (len(gb_only_prefixed) if hasattr(gb_only_prefixed, '__len__') else 0))&#10;    except Exception:&#10;        pass&#10;&#10;    # Evitar concatenar DataFrames vacíos (reduce warnings y asegura dtypes cohertes)&#10;    to_concat = [df for df in (merged_by_isbn.reset_index(drop=True) if isinstance(merged_by_isbn, pd.DataFrame) else merged_by_isbn,&#10;                               merged_by_key.reset_index(drop=True) if isinstance(merged_by_key, pd.DataFrame) else merged_by_key,&#10;                               gb_only_prefixed.reset_index(drop=True) if isinstance(gb_only_prefixed, pd.DataFrame) else gb_only_prefixed)&#10;                if isinstance(df, pd.DataFrame) and df.shape[0] &gt; 0]&#10;    if to_concat:&#10;        # diagnóstico: comprobar suma de filas antes de concatenar&#10;        component_counts = []&#10;        try:&#10;            component_counts = [len(df) for df in to_concat]&#10;            sum_rows = sum(component_counts)&#10;            logger.info('Component row counts before concat: %s; sum=%d', component_counts, sum_rows)&#10;        except Exception:&#10;            sum_rows = None&#10;        merged = pd.concat(to_concat, ignore_index=True, sort=False)&#10;        # diagnóstico adicional: calcular expected a partir de componentes reales&#10;        try:&#10;            expected = (len(merged_by_isbn) if isinstance(merged_by_isbn, pd.DataFrame) else 0) + (len(merged_by_key) if isinstance(merged_by_key, pd.DataFrame) else 0) + (len(gb_only_prefixed) if isinstance(gb_only_prefixed, pd.DataFrame) else 0)&#10;            logger.info('Expected merged rows (by_isbn + by_key + gb_only) = %d; actual merged = %d', expected, len(merged))&#10;            if expected is not None and len(merged) != expected:&#10;                logger.warning('Concatenación: filas inesperadas. expected=%s actual=%d; component_counts=%s', str(expected), len(merged), component_counts)&#10;        except Exception:&#10;            logger.debug('No se pudo calcular expected rows con componentes')&#10;        try:&#10;             if sum_rows is not None and len(merged) != sum_rows:&#10;                 logger.warning('Concatenación resultó en diferente número de filas: expected_sum=%s actual=%d', str(sum_rows), len(merged))&#10;                 # mostrar primeros indices y muestras para diagnóstico&#10;                 try:&#10;                     for i, df in enumerate(to_concat):&#10;                         logger.debug('component %d head:\n%s', i, df.head(3).to_string())&#10;                 except Exception:&#10;                     pass&#10;        except Exception:&#10;            pass&#10;    else:&#10;         # DataFrame vacío con al menos columnas esperadas&#10;         merged = pd.DataFrame()&#10;&#10;    # asegurar existencia de columna _source_type para downstream&#10;    if '_source_type' not in merged.columns:&#10;        merged['_source_type'] = None&#10;&#10;    return merged&#10;&#10;&#10;def _safe_write_parquet(df: pd.DataFrame, path: Path, index: bool = False) -&gt; None:&#10;    &quot;&quot;&quot;Escribe DataFrame a parquet de forma robusta.&#10;&#10;    Intenta escribir con pyarrow; si falla por tipos no esperados, normaliza&#10;    columnas de tipo objeto serializando dicts/listas a JSON y reintenta. Si sigue fallando, fuerza toda la tabla a&#10;    strings como último recurso.&#10;&#10;    Escribe a un fichero temporal y luego reemplaza el destino para evitar corrupciones&#10;    y problemas de concurrencia/locks en Windows. También intenta eliminar el destino&#10;    previo antes de reemplazarlo si surge un permiso.&#10;    &quot;&quot;&quot;&#10;    import os&#10;    import uuid&#10;    import gc&#10;&#10;    # Asegurar directorio destino&#10;    try:&#10;        path.parent.mkdir(parents=True, exist_ok=True)&#10;    except Exception:&#10;        pass&#10;&#10;    # Crear temp_path con uuid para evitar colisiones en re-ejecuciones simultáneas&#10;    temp_path = path.with_name(path.name + f&quot;.{uuid.uuid4().hex}.tmp&quot;)&#10;    # eliminar temp previo si existe (poco probable por uuid)&#10;    try:&#10;        if temp_path.exists():&#10;            temp_path.unlink()&#10;    except Exception:&#10;        pass&#10;&#10;    # Intentar eliminar destino previo antes de escribir para evitar locks en Windows&#10;    def _try_remove_target(p: Path):&#10;        try:&#10;            if p.exists():&#10;                p.unlink()&#10;                return True&#10;        except Exception:&#10;            # intentar renombrar como respaldo (si unlink falla por lock)&#10;            try:&#10;                backup = p.with_name(p.name + &quot;.backup&quot;)&#10;                if backup.exists():&#10;                    backup.unlink()&#10;                p.replace(backup)&#10;                return True&#10;            except Exception:&#10;                return False&#10;        return False&#10;&#10;    def _try_write(df_to_write: pd.DataFrame, target: Path) -&gt; bool:&#10;        try:&#10;            # Forzar engine pyarrow y cerrar recursos inmediatamente&#10;            df_to_write.to_parquet(target, index=index, engine='pyarrow')&#10;            # forzar GC para que pyarrow libere recursos en Windows&#10;            gc.collect()&#10;            return True&#10;        except Exception as e:&#10;            logger.debug('to_parquet falló para %s: %s', target.name, str(e))&#10;            return False&#10;&#10;    # Primer intento: eliminar objetivo si posible (mejora para re-ejecuciones)&#10;    try:&#10;        _try_remove_target(path)&#10;    except Exception:&#10;        pass&#10;&#10;    # Primer intento: escribir directamente al archivo temporal&#10;    try:&#10;        if _try_write(df, temp_path):&#10;            try:&#10;                # Reemplazar atómicamente el destino final (os.replace es atómico en Windows)&#10;                try:&#10;                    os.replace(str(temp_path), str(path))&#10;                except Exception:&#10;                    # fallback a Path.replace&#10;                    temp_path.replace(path)&#10;                logger.info('Escrito %s (replace desde temp)', path.name)&#10;                return&#10;            except Exception as e:&#10;                logger.warning('Fallo al reemplazar %s desde temp: %s. Intentando escritura directa.', path.name, str(e))&#10;                # intentar escribir directamente al destino como último recurso&#10;                try:&#10;                    if _try_write(df, path):&#10;                        logger.info('Escrito %s (directo tras fallo replace)', path.name)&#10;                        try:&#10;                            if temp_path.exists():&#10;                                temp_path.unlink()&#10;                        except Exception:&#10;                            pass&#10;                        return&#10;                except Exception:&#10;                    pass&#10;    except Exception:&#10;        pass&#10;&#10;    logger.warning('Error escribiendo %s. Intentando normalizar columnas objeto.', path.name)&#10;    df2 = df.copy()&#10;    # detectar columnas obj y serializar elementos complejos&#10;    for col in df2.columns:&#10;        if df2[col].dtype == object or df2[col].dtype.name == 'bytes':&#10;            def _conv(v):&#10;                try:&#10;                    if v is None or (isinstance(v, float) and pd.isna(v)):&#10;                        return None&#10;                    # bytes -&gt; str&#10;                    if isinstance(v, (bytes, bytearray)):&#10;                        try:&#10;                            return v.decode('utf-8')&#10;                        except Exception:&#10;                            return str(v)&#10;                    # listas/dicts -&gt; JSON string (preserva estructura)&#10;                    if isinstance(v, (dict, list)):&#10;                        try:&#10;                            return json.dumps(v, ensure_ascii=False)&#10;                        except Exception:&#10;                            return str(v)&#10;                    # numpy types -&gt; native python then str&#10;                    if hasattr(v, 'tolist'):&#10;                        try:&#10;                            return str(v.tolist())&#10;                        except Exception:&#10;                            pass&#10;                    # otros tipos (int, float, etc.) -&gt; str&#10;                    return str(v)&#10;                except Exception:&#10;                    return str(v)&#10;            try:&#10;                df2[col] = df2[col].apply(_conv)&#10;            except Exception:&#10;                # último recurso: convertir toda la columna a string&#10;                try:&#10;                    df2[col] = df2[col].astype(str)&#10;                except Exception:&#10;                    df2[col] = df2[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else str(v))&#10;    # Intentar escribir df2 a temp y reemplazar&#10;    try:&#10;        if _try_write(df2, temp_path):&#10;            try:&#10;                try:&#10;                    os.replace(str(temp_path), str(path))&#10;                except Exception:&#10;                    temp_path.replace(path)&#10;                logger.info('Escrito (normalizado) %s', path.name)&#10;                return&#10;            except Exception as e2:&#10;                logger.warning('Fallo al reemplazar %s tras normalizar: %s', path.name, str(e2))&#10;                try:&#10;                    if _try_write(df2, path):&#10;                        logger.info('Escrito %s (directo, normalizado)', path.name)&#10;                        try:&#10;                            if temp_path.exists():&#10;                                temp_path.unlink()&#10;                        except Exception:&#10;                            pass&#10;                        return&#10;                except Exception:&#10;                    pass&#10;    except Exception:&#10;        pass&#10;&#10;    logger.warning('Fallo escribiendo %s tras normalizar columnas. Forzando todo a strings.', path.name)&#10;    # Último intento: forzar todas las columnas a strings (None -&gt; None)&#10;    df3 = df.copy()&#10;    for col in df3.columns:&#10;        try:&#10;            df3[col] = df3[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else str(v))&#10;        except Exception:&#10;            try:&#10;                df3[col] = df3[col].astype(str)&#10;            except Exception:&#10;                df3[col] = df3[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else str(v))&#10;    # Escribir df3 a temp y reemplazar&#10;    try:&#10;        if _try_write(df3, temp_path):&#10;            try:&#10;                try:&#10;                    os.replace(str(temp_path), str(path))&#10;                except Exception:&#10;                    temp_path.replace(path)&#10;                logger.info('Escrito (fallback strings) %s', path.name)&#10;                return&#10;            except Exception as e3:&#10;                # último recurso: escribir directo&#10;                try:&#10;                    df3.to_parquet(path, index=index, engine='pyarrow')&#10;                    logger.info('Escrito (fallback directo) %s', path.name)&#10;                    try:&#10;                        if temp_path.exists():&#10;                            temp_path.unlink()&#10;                    except Exception:&#10;                        pass&#10;                    return&#10;                except Exception as e4:&#10;                    logger.error('Fallo definitivo al escribir %s: %s', path.name, str(e4))&#10;                    raise&#10;    except Exception as e:&#10;        logger.error('Fallo definitivo al escribir %s: %s', path.name, str(e))&#10;        raise&#10;&#10;def safe_compute_quality_metrics(rows):&#10;    &quot;&quot;&quot;Wrapper seguro para compute_quality_metrics: si falla la función original,&#10;    devuelve métricas mínimas que permiten continuar y rastrear el error.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        return compute_quality_metrics(rows)&#10;    except Exception as e:&#10;        logger.warning(&quot;safe_compute_quality_metrics: compute_quality_metrics falló: %s&quot;, str(e))&#10;        # Fallback mínimo&#10;        total = len(rows) if isinstance(rows, list) else 0&#10;        nulos = {}&#10;        try:&#10;            cols = [&quot;book_id&quot;, &quot;titulo&quot;, &quot;autor_principal&quot;, &quot;isbn13&quot;, &quot;fecha_publicacion&quot;, &quot;idioma&quot;, &quot;precio&quot;, &quot;moneda&quot;]&#10;            nulos = nulls_by_column(rows, cols) if rows else {c: 0 for c in cols}&#10;        except Exception:&#10;            nulos = {}&#10;        return {&#10;            &quot;filas_totales&quot;: total,&#10;            &quot;nulos_por_campo&quot;: nulos,&#10;            &quot;porcentaje_fechas_validas&quot;: 0.0,&#10;            &quot;porcentaje_idiomas_validos&quot;: 0.0,&#10;            &quot;porcentaje_monedas_validas&quot;: 0.0,&#10;            &quot;porcentaje_isbn13_validos&quot;: 0.0,&#10;            &quot;completitud_promedio&quot;: 0.0,&#10;            &quot;duplicados_isbn13&quot;: 0,&#10;            &quot;duplicados_book_id&quot;: 0,&#10;            &quot;filas_por_fuente&quot;: {},&#10;        }&#10;&#10;def _canonicalize_merged_columns(merged: pd.DataFrame) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;Asegura nombres de columnas consistentes y tipos básicos para downstream.&#10;    - Normaliza sufijos si existen mezclas (_gr/_gb)&#10;    - Crea columnas _source_title/_source_author auxiliares si procede&#10;    KEYWORDS: CANONICALIZE_COLUMNS&#10;    &quot;&quot;&quot;&#10;    if not isinstance(merged, pd.DataFrame):&#10;        return merged&#10;    df = merged.copy()&#10;    # eliminar duplicados accidentales en nombres (ej. col y col_gr ambos presentes)&#10;    for col in list(df.columns):&#10;        if col.endswith('_gr') or col.endswith('_gb'):&#10;            base = col[:-3]&#10;            # si existe la columna base y la columna sufijada tiene datos más completos, preferir sufijo&#10;            try:&#10;                if base in df.columns:&#10;                    # preferir valores no nulos de sufijo sobre base&#10;                    df[base] = df[base].where(df[base].notna(), df[col])&#10;            except Exception:&#10;                pass&#10;    # asegurar columnas clave presentes&#10;    for expected in ['titulo','autor_principal','isbn13','isbn10','precio','moneda','fecha_publicacion','autores','categoria']:&#10;        if expected not in df.columns:&#10;            df[expected] = None&#10;    return df&#10;&#10;&#10;def _apply_semantic_normalization(df: pd.DataFrame) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;Normaliza fechas, idiomas, moneda y precio, y añade flags de validez.&#10;    Operación vectorizada en la medida de lo posible para rendimiento.&#10;    KEYWORDS: NORMALIZE_SEMANTIC&#10;    &quot;&quot;&quot;&#10;    if not isinstance(df, pd.DataFrame):&#10;        return df&#10;    out = df.copy()&#10;    # Normalizar fecha_publicacion -&gt; ISO y marcar parcial&#10;    if 'fecha_publicacion' in out.columns:&#10;        iso_dates = []&#10;        parcial_flags = []&#10;        valid_flags = []&#10;        for v in out['fecha_publicacion'].fillna('').astype(str).tolist():&#10;            if not v or v.strip() == '' or v == 'nan':&#10;                iso_dates.append(None)&#10;                parcial_flags.append(False)&#10;                valid_flags.append(False)&#10;            else:&#10;                try:&#10;                    iso, parcial = parse_date_to_iso(v)&#10;                    iso_dates.append(iso)&#10;                    parcial_flags.append(bool(parcial))&#10;                    valid_flags.append(bool(iso is not None))&#10;                except Exception:&#10;                    iso_dates.append(None)&#10;                    parcial_flags.append(False)&#10;                    valid_flags.append(False)&#10;        out['fecha_publicacion'] = iso_dates&#10;        out['fecha_publicacion_parcial'] = parcial_flags&#10;        out['fecha_publicacion_valida'] = valid_flags&#10;    else:&#10;        out['fecha_publicacion_parcial'] = False&#10;        out['fecha_publicacion_valida'] = False&#10;&#10;    # Normalizar idioma&#10;    if 'idioma' in out.columns:&#10;        norm_langs = []&#10;        lang_valid = []&#10;        for v in out['idioma'].fillna('').astype(str).tolist():&#10;            if not v or v.strip() == '' or v == 'nan':&#10;                norm_langs.append(None)&#10;                lang_valid.append(False)&#10;            else:&#10;                try:&#10;                    nl = normalize_language(v)&#10;                    ok = validate_language(nl)&#10;                    norm_langs.append(nl)&#10;                    lang_valid.append(bool(ok))&#10;                except Exception:&#10;                    norm_langs.append(None)&#10;                    lang_valid.append(False)&#10;        out['idioma'] = norm_langs&#10;        out['idioma_valido'] = lang_valid&#10;    else:&#10;        out['idioma_valido'] = False&#10;&#10;    # Normalizar moneda&#10;    if 'moneda' in out.columns:&#10;        norm_cur = []&#10;        cur_valid = []&#10;        for v in out['moneda'].fillna('').astype(str).tolist():&#10;            if not v or v.strip() == '' or v == 'nan':&#10;                norm_cur.append(None)&#10;                cur_valid.append(False)&#10;            else:&#10;                try:&#10;                    nc = normalize_currency(v)&#10;                    ok = validate_currency(nc) if nc else False&#10;                    norm_cur.append(nc)&#10;                    cur_valid.append(bool(ok))&#10;                except Exception:&#10;                    norm_cur.append(None)&#10;                    cur_valid.append(False)&#10;        out['moneda'] = norm_cur&#10;        out['moneda_valida'] = cur_valid&#10;    else:&#10;        out['moneda_valida'] = False&#10;&#10;    # Normalizar precio -&gt; float coercion&#10;    if 'precio' in out.columns:&#10;        precs = []&#10;        for v in out['precio'].tolist():&#10;            if v is None or (isinstance(v, float) and pd.isna(v)) or (isinstance(v, str) and v.strip() == ''):&#10;                precs.append(None)&#10;            else:&#10;                try:&#10;                    # limpiar comas y espacios; forzar punto decimal&#10;                    s = str(v).strip()&#10;                    s = s.replace(',', '.')&#10;                    # eliminar símbolos excepto digits, dot and minus&#10;                    s2 = ''.join([c for c in s if (c.isdigit() or c in '.-')])&#10;                    precs.append(float(s2))&#10;                except Exception:&#10;                    precs.append(None)&#10;        out['precio'] = precs&#10;    return out&#10;&#10;&#10;def integrate() -&gt; None:&#10;    &quot;&quot;&quot;Integración completa: carga, normalización, fusión y escritura de artefactos estándar.&#10;    Ejecutar en el orden correcto es crítico: primero se cargan y normalizan las fuentes,&#10;    luego se fusionan, se generan métricas de calidad y finalmente se escriben los resultados.&#10;    &quot;&quot;&quot;&#10;    global gr_n, gb_n, price_map, detail, assertions&#10;&#10;    # Inicializaciones defensivas para evitar referencias no resueltas en flujos excepcionales&#10;    dim = pd.DataFrame()&#10;    dim_for_metrics = pd.DataFrame()&#10;    detail = pd.DataFrame()&#10;    assertions = {}&#10;&#10;    # Asegurar directorios de logs de trabajo (work/logs/..)&#10;    try:&#10;        ensure_work_dirs()&#10;    except Exception:&#10;        # no bloquear si no existe work/utils_logging&#10;        pass&#10;&#10;    # Eliminar artefactos previos para evitar problemas de locks en Windows y re-ejecuciones&#10;    try:&#10;        prev_dim = STANDARD / 'dim_book.parquet'&#10;        prev_detail = STANDARD / 'book_source_detail.parquet'&#10;        if prev_dim.exists():&#10;            try:&#10;                prev_dim.unlink()&#10;                logger.info('Eliminado artefacto previo: %s', prev_dim.name)&#10;            except Exception:&#10;                logger.debug('No se pudo eliminar previo dim_book.parquet', exc_info=True)&#10;        if prev_detail.exists():&#10;            try:&#10;                prev_detail.unlink()&#10;                logger.info('Eliminado artefacto previo: %s', prev_detail.name)&#10;            except Exception:&#10;                logger.debug('No se pudo eliminar previo book_source_detail.parquet', exc_info=True)&#10;    except Exception:&#10;        pass&#10;&#10;    # Carga inicial de fuentes&#10;    gr_n, gb_n = _load_sources()&#10;&#10;    # Normalización básica&#10;    gr_n, gb_n = _normalize_frames(gr_n, gb_n)&#10;&#10;    # Merge y asignación de ISBN desde candidatos&#10;    merged = _merge_sources(gr_n, gb_n)&#10;&#10;    # Normalizar columnas canónicas para prevenir errores posteriores&#10;    try:&#10;        merged = _canonicalize_merged_columns(merged)&#10;    except Exception:&#10;        logger.debug('Fallo al canonicalizar columnas de merged; se mantiene objeto original', exc_info=True)&#10;&#10;    # APLICAR NORMALIZACIÓN SEMÁNTICA: fechas ISO, idioma BCP-47, moneda ISO-4217, precio numérico&#10;    try:&#10;        merged = _apply_semantic_normalization(merged)&#10;    except Exception:&#10;        logger.debug('Normalización semántica falló; se continúa con merged original', exc_info=True)&#10;&#10;    # Unificar columnas base (coalesce) para que existan nombres sin sufijos (titulo, autor_principal, isbn13, etc.)&#10;    try:&#10;        base_cols = [&#10;            'titulo','autor_principal','autores','editorial','anio_publicacion','fecha_publicacion','idioma','isbn10','isbn13',&#10;            'precio','moneda','categoria','ts_ultima_actualizacion','isbn13_valido','idioma_valido','moneda_valida','fecha_publicacion_valida','fecha_publicacion_parcial','gb_match_score'&#10;        ]&#10;        for col in base_cols:&#10;            series = None&#10;            # considerar tanto formas con sufijos como las canónicas ya presentes&#10;            candidates = [col, f&quot;{col}_gr&quot;, f&quot;{col}_gb&quot;, f&quot;{col}_gr&quot;.replace('_gr','_gr'), f&quot;{col}_gb&quot;.replace('_gb','_gb')]&#10;            for key in candidates:&#10;                if key in merged.columns:&#10;                    s = merged[key]&#10;                    if series is None:&#10;                        series = s.copy()&#10;                    else:&#10;                        try:&#10;                            series = series.fillna(s)&#10;                        except Exception:&#10;                            series = series.where(series.notna(), s)&#10;            if series is None:&#10;                merged[col] = pd.Series([None] * len(merged), index=merged.index)&#10;            else:&#10;                merged[col] = series&#10;    except Exception:&#10;        logger.debug('Coalesce de columnas base falló, se continúa con merged tal cual', exc_info=True)&#10;&#10;    # Separar merged en dos vistas: una completa para detalle (detail) y otra filtrada para construir dim (excluir gb_only)&#10;    try:&#10;        merged_for_dim = merged[merged['_source_type'] != 'gb_only'].copy() if isinstance(merged, pd.DataFrame) else pd.DataFrame()&#10;    except Exception:&#10;        merged_for_dim = merged.copy() if isinstance(merged, pd.DataFrame) else pd.DataFrame()&#10;&#10;    # ===== Construir dim canónico a partir de merged_for_dim =====&#10;    try:&#10;        # Intentar construir la tabla dimensional con la función dedicada&#10;        dim = _build_dim_from_merged(merged_for_dim)&#10;&#10;        # Asignar book_id: preferir isbn13 válido, sino canonical_key&#10;        try:&#10;            if 'isbn13' in dim.columns and 'isbn13_valido' in dim.columns:&#10;                dim['book_id'] = dim.apply(lambda r: r['isbn13'] if (pd.notna(r.get('isbn13')) and bool(r.get('isbn13_valido'))) else r.get('canonical_key'), axis=1)&#10;            elif 'canonical_key' in dim.columns:&#10;                dim['book_id'] = dim['canonical_key']&#10;            else:&#10;                dim['book_id'] = None&#10;        except Exception:&#10;            # fallback genérico&#10;            if 'canonical_key' in dim.columns:&#10;                dim['book_id'] = dim['canonical_key']&#10;            else:&#10;                dim['book_id'] = None&#10;&#10;        # Asegurar ts_ultima_actualizacion&#10;        try:&#10;            now_ts = datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;            if 'ts_ultima_actualizacion' not in dim.columns:&#10;                dim['ts_ultima_actualizacion'] = now_ts&#10;            else:&#10;                dim['ts_ultima_actualizacion'] = dim['ts_ultima_actualizacion'].fillna(now_ts)&#10;        except Exception:&#10;            dim['ts_ultima_actualizacion'] = datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;&#10;        # Deduplicar por book_id garantizando una fila por libro canónico&#10;        try:&#10;            if 'book_id' in dim.columns:&#10;                # preferir filas con más campos no nulos y con fuente_ganadora&#10;                # calcular score simple: número de campos no nulos&#10;                score_cols = ['titulo','autor_principal','editorial','fecha_publicacion','autores','precio','moneda']&#10;                dim['_completitud_tmp'] = dim[score_cols].notna().sum(axis=1)&#10;                # si existe gb_match_score, añadirlo al score&#10;                if 'gb_match_score' in dim.columns:&#10;                    try:&#10;                        dim['_gb_ms_tmp'] = pd.to_numeric(dim['gb_match_score'], errors='coerce').fillna(0.0)&#10;                        dim['_score_total'] = dim['_completitud_tmp'] + dim['_gb_ms_tmp']&#10;                    except Exception:&#10;                        dim['_score_total'] = dim['_completitud_tmp']&#10;                else:&#10;                    dim['_score_total'] = dim['_completitud_tmp']&#10;                dim = dim.sort_values(by=['_score_total'], ascending=False).drop_duplicates(subset=['book_id'], keep='first').reset_index(drop=True)&#10;                # eliminar columnas temporales&#10;                for _c in ['_completitud_tmp','_gb_ms_tmp','_score_total']:&#10;                    if _c in dim.columns:&#10;                        try:&#10;                            del dim[_c]&#10;                        except Exception:&#10;                            pass&#10;        except Exception:&#10;            # si falla deduplicación, no interrumpir; dim se mantiene&#10;            pass&#10;    except Exception as e:&#10;        # No bloquear pipeline si la construcción del dim falla; dejar que el flujo posterior use el fallback&#10;        logger.warning('No se pudo construir dim desde merged_for_dim: %s', str(e))&#10;        dim = pd.DataFrame()&#10;&#10;    # Registro de diagnóstico: tamaño de merged y unicidad de book_id antes de dedup&#10;    try:&#10;        logger.info(&#10;            'Merged rows (total) antes dedup: %d; merged_for_dim rows: %d',&#10;            0 if merged is None else (len(merged) if hasattr(merged, '__len__') else 0),&#10;            0 if merged_for_dim is None else (len(merged_for_dim) if hasattr(merged_for_dim, '__len__') else 0),&#10;        )&#10;        if isinstance(merged, pd.DataFrame) and not merged.empty:&#10;            # contar book_id posibles (puede no existir aún)&#10;            if 'book_id' in merged.columns:&#10;                counts = merged['book_id'].value_counts(dropna=False)&#10;                dup_groups = counts[counts &gt; 1]&#10;                logger.info('book_id únicos en merged: %d, grupos duplicados: %d', counts.shape[0], dup_groups.shape[0])&#10;                if not dup_groups.empty:&#10;                    logger.debug('book_id duplicados (muestra): %s', dup_groups.head(10).to_dict())&#10;            else:&#10;                logger.info('book_id no presente aún en merged')&#10;    except Exception:&#10;        logger.debug('Diagnóstico merged falló', exc_info=True)&#10;&#10;    # --- Aserciones soft y deduplicación ---&#10;    # Construir book_source_detail a partir de 'merged' y aplicar reglas de exclusión suaves&#10;    try:&#10;        # convertir merged (DataFrame) en uno con columnas esperadas&#10;        # detail = merged.copy()&#10;        # fila original (row_number) para trazabilidad&#10;        # detail['row_number'] = detail.index + 1&#10;        # detail['book_id_candidato'] = detail.get('book_id')&#10;        # detail['ts_ingesta'] = detail.get('ts_ultima_actualizacion') if 'ts_ultima_actualizacion' in detail.columns else datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;&#10;        # Construir `book_source_detail` preservando las filas originales de landing&#10;        # y rellenando campos de Goodreads desde GoogleBooks cuando sea posible.&#10;        # _build_book_source_detail devuelve un DataFrame con las columnas exactas&#10;        # requeridas y `ts_ingest` con la hora de ejecución.&#10;        try:&#10;            detail = _build_book_source_detail(gr_n, gb_n)&#10;        except Exception as _e:&#10;            logger.warning('Fallo construyendo book_source_detail desde landing: %s. Usando merged como fallback.', str(_e))&#10;            detail = merged.copy()&#10;            # mantener compatibilidad: asignar row_number y ts_ingesta si faltan&#10;            if 'row_number' not in detail.columns:&#10;                detail['row_number'] = detail.index + 1&#10;            if 'book_id_candidato' not in detail.columns:&#10;                detail['book_id_candidato'] = detail.get('book_id')&#10;            if 'ts_ingest' not in detail.columns:&#10;                detail['ts_ingest'] = detail.get('ts_ultima_actualizacion') if 'ts_ultima_actualizacion' in detail.columns else datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;    except Exception as e:&#10;        logger.warning('Error en fase de deduplicación/aserciones: %s', str(e))&#10;        logger.debug(traceback.format_exc())&#10;        # fallback: usar merged como dim y un detail mínimo&#10;        dim = merged.copy()&#10;        detail = merged.copy()&#10;        detail['row_number'] = detail.index + 1&#10;        detail['book_id_candidato'] = detail.get('book_id')&#10;        detail['ts_ingesta'] = detail.get('ts_ultima_actualizacion') if 'ts_ultima_actualizacion' in detail.columns else datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;        detail['valid'] = True&#10;        detail['exclude_reason'] = None&#10;&#10;    # APLICAR LIMPIEZA DE REFERENCIAS ANTES DE ESCRIBIR PARA EVITAR BLOQUEOS EN WINDOWS&#10;    try:&#10;        import gc&#10;        # eliminar referencias pesadas temporales&#10;        for _n in ('merged_local','merged_by_isbn','merged_by_key','gb_only_prefixed','to_concat','merged'):&#10;            try:&#10;                if _n in locals():&#10;                    del locals()[_n]&#10;            except Exception:&#10;                pass&#10;        gc.collect()&#10;    except Exception:&#10;        pass&#10;&#10;    # Métricas de calidad&#10;    records_sanitized: List[Mapping[str, object]] = []&#10;    try:&#10;        # usar dim deduplicado y final para métricas (asegurar 1 fila por libro canónico)&#10;        try:&#10;            # deduplicar por isbn13 preferente, si no usar book_id&#10;            dim_for_metrics = dim.copy()&#10;            if 'isbn13' in dim_for_metrics.columns and dim_for_metrics['isbn13'].notna().any():&#10;                dim_for_metrics = dim_for_metrics.sort_values(by=['isbn13','gb_match_score'], ascending=[True, False]).drop_duplicates(subset=['isbn13'], keep='first').reset_index(drop=True)&#10;            else:&#10;                dim_for_metrics = dim_for_metrics.drop_duplicates(subset=['book_id'], keep='first').reset_index(drop=True)&#10;            records = dim_for_metrics.to_dict(orient='records')&#10;        except Exception as e:&#10;            logger.warning('dim.to_dict falló para métricas: %s. Intentando fallback row-wise.', str(e))&#10;            records = []&#10;            try:&#10;                for _, row in dim.iterrows():&#10;                    try:&#10;                        # construir dict simple con columnas planas&#10;                        rec = {c: _sanitize_value(row.get(c)) for c in dim.columns}&#10;                        records.append(rec)&#10;                    except Exception:&#10;                        records.append({})&#10;            except Exception:&#10;                records = []&#10;        # Sanitizar registros para evitar tipos complejos que rompan compute_quality_metrics&#10;        try:&#10;            records_sanitized = _sanitize_records_for_metrics(records)&#10;        except Exception as e:&#10;            logger.warning('Sanitización de records falló: %s. Intentando sanitizar manualmente.', str(e))&#10;            records_sanitized = []&#10;            for r in (records or []):&#10;                if isinstance(r, Mapping):&#10;                    row = {}&#10;                    for k, v in r.items():&#10;                        try:&#10;                            row[k] = _sanitize_value(v)&#10;                        except Exception:&#10;                            row[k] = None&#10;                    records_sanitized.append(row)&#10;                else:&#10;                    try:&#10;                        records_sanitized.append({'raw': _sanitize_value(r)})&#10;                    except Exception:&#10;                        records_sanitized.append({})&#10;        # usar wrapper seguro para evitar que una excepción interrumpa la escritura&#10;        assertions = safe_compute_quality_metrics(records_sanitized)&#10;        # Forzar valores clave coherentes: filas_totales = número de filas del dim final&#10;        try:&#10;            assertions['filas_totales'] = len(records_sanitized)&#10;        except Exception:&#10;            pass&#10;        # filas_por_fuente: contar por 'fuente_ganadora' en dim_for_metrics si existe&#10;        try:&#10;            if isinstance(dim_for_metrics, pd.DataFrame) and 'fuente_ganadora' in dim_for_metrics.columns:&#10;                fps = dim_for_metrics['fuente_ganadora'].fillna('unknown').value_counts().to_dict()&#10;                assertions['filas_por_fuente'] = fps&#10;            else:&#10;                # fallback: contar por source_name columns in detail&#10;                assertions.setdefault('filas_por_fuente', {})&#10;        except Exception:&#10;            pass&#10;    except Exception as e:&#10;        logger.warning('Error calculando métricas de calidad: %s', str(e))&#10;        logger.debug(traceback.format_exc())&#10;        assertions = safe_compute_quality_metrics([])&#10;&#10;    # Registrar una entrada mínima en logs/rules para trazabilidad (evita carpeta vacía)&#10;    try:&#10;        log_rule_jsonl({&#10;            &quot;event&quot;: &quot;integrate_run_summary&quot;,&#10;            &quot;ts_utc&quot;: datetime.now(timezone.utc).isoformat(timespec='seconds'),&#10;            &quot;records_input&quot;: assertions.get('filas_totales') if isinstance(assertions, dict) else None,&#10;            &quot;notes&quot;: &quot;integrate_pipeline executed&quot;,&#10;        })&#10;    except Exception:&#10;        pass&#10;&#10;    # Escritura de artefactos estándar&#10;    try:&#10;        # normalizar tipos en dim para evitar problemas con pyarrow&#10;        for col in dim.columns:&#10;            if dim[col].dtype == object:&#10;                # serializar dict/list/bytes&#10;                dim[col] = dim[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else (v.decode('utf-8') if isinstance(v, (bytes, bytearray)) else (json.dumps(v, ensure_ascii=False) if isinstance(v, (dict, list)) else v)))&#10;        # liberar referencias pesadas antes de escribir para evitar locks en Windows&#10;        try:&#10;            import gc&#10;            # eliminar referencias que ya no se necesitan&#10;            for _name in ('gr_n', 'gb_n', 'merged', 'merged_local', 'merged_by_isbn', 'merged_by_key', 'gb_only'):&#10;                try:&#10;                    if _name in globals():&#10;                        del globals()[_name]&#10;                except Exception:&#10;                    pass&#10;            gc.collect()&#10;        except Exception:&#10;            pass&#10;        _safe_write_parquet(dim, STANDARD / 'dim_book.parquet', index=False)&#10;    except Exception as e:&#10;        logger.error('Fallo al escribir dim_book.parquet: %s', str(e))&#10;        raise&#10;&#10;    try:&#10;        if not isinstance(detail, pd.DataFrame):&#10;            detail = pd.DataFrame(detail)&#10;        # normalizar detail: asegurar tipos simples&#10;        for col in detail.columns:&#10;            if detail[col].dtype == object:&#10;                # aplicar _sanitize_value por elemento para evitar listas/dicts/bytes&#10;                try:&#10;                    detail[col] = detail[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else _sanitize_value(v))&#10;                except Exception:&#10;                    # último recurso: convertir a string seguro&#10;                    detail[col] = detail[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else str(v))&#10;        # asegurar row_number entero&#10;        if 'row_number' in detail.columns:&#10;            try:&#10;                detail['row_number'] = pd.to_numeric(detail['row_number'], errors='coerce').astype('Int64')&#10;            except Exception:&#10;                pass&#10;        _safe_write_parquet(detail, STANDARD / 'book_source_detail.parquet', index=False)&#10;    except Exception as e:&#10;        logger.error('Fallo al escribir book_source_detail.parquet: %s', str(e))&#10;        # intentar escribir una versión reducida para diagnóstico&#10;        try:&#10;            diag = detail.copy()&#10;            # limitar columnas a las más relevantes y sanitizar&#10;            keep = [c for c in ['row_number','book_id_candidato','titulo','autor_principal','isbn13','fecha_publicacion','precio','moneda','valid','exclude_reason'] if c in diag.columns]&#10;            diag = diag[keep]&#10;            for col in diag.columns:&#10;                diag[col] = diag[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else _sanitize_value(v))&#10;            _safe_write_parquet(diag, STANDARD / 'book_source_detail.parquet', index=False)&#10;            logger.info('Escrito book_source_detail.parquet (diagnóstico reducido)')&#10;        except Exception as e2:&#10;            logger.error('Fallo final al escribir book_source_detail.parquet (diagnóstico): %s', str(e2))&#10;            raise&#10;&#10;    try:&#10;        with open(DOCS / 'quality_metrics.json', 'w', encoding='utf-8') as f:&#10;            json.dump(assertions, f, ensure_ascii=False, indent=2)&#10;    except Exception as e:&#10;        logger.warning('No se pudo escribir quality_metrics.json: %s', str(e))&#10;&#10;    logger.info('Escrito dim_book.parquet y book_source_detail.parquet; métricas en quality_metrics.json')&#10;&#10;&#10;def main() -&gt; None:&#10;    integrate()&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;&#10;def _coalesce_columns(df: pd.DataFrame, col: str, suffixes: List[str] = None) -&gt; pd.Series:&#10;    # Intenta combinar valores de columnas con sufijos diferentes en una sola columna.&#10;    # Prioriza valores no nulos de columnas con sufijo sobre la columna base.&#10;    if suffixes is None:&#10;        suffixes = ['_gr', '_gb']&#10;    candidates = [col] + [f&quot;{col}{s}&quot; for s in suffixes]&#10;    series = None&#10;    for c in candidates:&#10;        if c in df.columns:&#10;            s = df[c]&#10;            if series is None:&#10;                series = s.astype(object)&#10;            else:&#10;                try:&#10;                    series = series.combine_first(s)&#10;                except Exception:&#10;                    series = series.where(series.notna(), s)&#10;    if series is None:&#10;        series = pd.Series([None] * len(df), index=df.index, dtype=object)&#10;    return series&#10;&#10;&#10;def _merge_list_fields(val_a, val_b):&#10;    # Mezcla dos campos de lista (separados por ';') o valores escalares, eliminando duplicados.&#10;    try:&#10;        a = []&#10;        b = []&#10;        if val_a is not None and (not (isinstance(val_a, float) and pd.isna(val_a))):&#10;            if isinstance(val_a, str):&#10;                a = [x.strip() for x in val_a.split(';') if x.strip()]&#10;            elif isinstance(val_a, (list, tuple)):&#10;                a = [str(x).strip() for x in val_a if x is not None]&#10;        if val_b is not None and (not (isinstance(val_b, float) and pd.isna(val_b))):&#10;            if isinstance(val_b, str):&#10;                b = [x.strip() for x in val_b.split(';') if x.strip()]&#10;            elif isinstance(val_b, (list, tuple)):&#10;                b = [str(x).strip() for x in val_b if x is not None]&#10;        merged = []&#10;        for x in a + b:&#10;            if x and x not in merged:&#10;                merged.append(x)&#10;        if merged:&#10;            return ';'.join(merged)&#10;    except Exception:&#10;        pass&#10;    return None&#10;&#10;&#10;def _compute_provenance(row: pd.Series, base_fields: List[str]) -&gt; str:&#10;    # Computa la procedencia de los datos en una fila dada, basado en los campos base.&#10;    prov = {}&#10;    try:&#10;        for f in base_fields:&#10;            src = None&#10;            gr_key = f&quot;{f}_gr&quot;&#10;            gb_key = f&quot;{f}_gb&quot;&#10;            if gr_key in row.index and pd.notna(row.get(gr_key)) and str(row.get(gr_key)).strip() != '':&#10;                src = 'goodreads'&#10;            elif gb_key in row.index and pd.notna(row.get(gb_key)) and str(row.get(gb_key)).strip() != '':&#10;                src = 'google_books'&#10;            else:&#10;                if f in row.index and pd.notna(row.get(f)) and str(row.get(f)).strip() != '':&#10;                    st = row.get('_source_type')&#10;                    if isinstance(st, str) and st.startswith('merged_by_'):&#10;                        src = 'merged'&#10;                    elif st == 'gb_only':&#10;                        src = 'google_books'&#10;                    elif st == 'gr_only':&#10;                        src = 'goodreads'&#10;                    else:&#10;                        src = 'unknown'&#10;            prov[f] = src&#10;    except Exception:&#10;        for f in base_fields:&#10;            prov[f] = None&#10;    try:&#10;        return json.dumps(prov, ensure_ascii=False)&#10;    except Exception:&#10;        return json.dumps({f: prov.get(f) for f in base_fields}, ensure_ascii=False)&#10;&#10;&#10;def _build_book_source_detail(gr: pd.DataFrame, gb: pd.DataFrame) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;Construye el detalle de la fuente de los libros a partir de las fuentes de Goodreads y Google Books.&#10;&#10;    Combina registros de ambas fuentes, asignando campos comunes y marcando la procedencia.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        ts = datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;        gr_copy = gr.copy()&#10;        # Asegurar nombres de columna consistentes (titulo, autor_principal, isbn13, isbn10, fecha_publicacion)&#10;        if 'title' in gr_copy.columns and 'titulo' not in gr_copy.columns:&#10;            gr_copy.rename(columns={'title': 'titulo'}, inplace=True)&#10;        if 'author' in gr_copy.columns and 'autor_principal' not in gr_copy.columns:&#10;            gr_copy.rename(columns={'author': 'autor_principal'}, inplace=True)&#10;&#10;        if 'row_number' not in gr_copy.columns:&#10;            try:&#10;                gr_copy['row_number'] = (gr_copy.index.astype(int) + 1)&#10;            except Exception:&#10;                gr_copy['row_number'] = list(range(1, len(gr_copy) + 1))&#10;        gr_copy['source_name'] = 'goodreads'&#10;        gr_copy['source_file'] = str((LANDING / 'goodreads_books.json').name)&#10;        gr_copy['ts_ingest'] = ts&#10;        # book_id_candidato: preferir isbn13 si existe&#10;        gr_copy['book_id_candidato'] = gr_copy.get('isbn13')&#10;        if 'isbn13' in gr_copy.columns:&#10;            gr_copy['isbn13_valido'] = gr_copy['isbn13'].apply(lambda v: bool(is_valid_isbn13(str(v))) if pd.notna(v) and str(v).strip() != '' else False)&#10;        else:&#10;            gr_copy['isbn13_valido'] = False&#10;        gr_copy['fecha_publicacion_valida'] = gr_copy.get('fecha_publicacion').apply(lambda v: bool(parse_date_to_iso(v)[0]) if pd.notna(v) and str(v).strip() != '' else False) if 'fecha_publicacion' in gr_copy.columns else False&#10;        # Corregir nombre de flag para idioma (consistencia)&#10;        gr_copy['idioma_valido'] = False&#10;        gr_copy['moneda_valida'] = False&#10;&#10;        gb_copy = gb.copy()&#10;        # Asegurar nombres de columna en gb&#10;        if 'title' in gb_copy.columns and 'titulo' not in gb_copy.columns:&#10;            gb_copy.rename(columns={'title': 'titulo'}, inplace=True)&#10;        if 'authors' in gb_copy.columns and 'autores' not in gb_copy.columns:&#10;            gb_copy.rename(columns={'authors': 'autores'}, inplace=True)&#10;        if '_csv_row' in gb_copy.columns:&#10;            gb_copy['row_number'] = gb_copy['_csv_row']&#10;        else:&#10;            try:&#10;                gb_copy['row_number'] = (gb_copy.index.astype(int) + 1)&#10;            except Exception:&#10;                gb_copy['row_number'] = list(range(1, len(gb_copy) + 1))&#10;        gb_copy['source_name'] = 'google_books'&#10;        gb_copy['source_file'] = str((LANDING / 'googlebooks_books.csv').name)&#10;        gb_copy['ts_ingest'] = ts&#10;        gb_copy['book_id_candidato'] = gb_copy.get('isbn13')&#10;        if 'isbn13' in gb_copy.columns:&#10;            gb_copy['isbn13_valido'] = gb_copy['isbn13'].apply(lambda v: bool(is_valid_isbn13(str(v))) if pd.notna(v) and str(v).strip() != '' else False)&#10;        else:&#10;            gb_copy['isbn13_valido'] = False&#10;        gb_copy['fecha_publicacion_valida'] = gb_copy.get('fecha_publicacion').apply(lambda v: bool(parse_date_to_iso(v)[0]) if pd.notna(v) and str(v).strip() != '' else False) if 'fecha_publicacion' in gb_copy.columns else False&#10;        if 'idioma' in gb_copy.columns:&#10;            gb_copy['idioma_valido'] = gb_copy['idioma'].apply(lambda v: bool(validate_language(normalize_language(v))) if pd.notna(v) and str(v).strip() != '' else False)&#10;        else:&#10;            gb_copy['idioma_valido'] = False&#10;        if 'moneda' in gb_copy.columns:&#10;            gb_copy['moneda_valida'] = gb_copy['moneda'].apply(lambda v: bool(validate_currency(normalize_currency(v))) if pd.notna(v) and str(v).strip() != '' else False)&#10;        else:&#10;            gb_copy['moneda_valida'] = False&#10;&#10;        if 'autores' in gb_copy.columns:&#10;            gb_copy['autores'] = gb_copy['autores'].apply(lambda x: ';'.join(uniq_preserve(listify(x))) if pd.notna(x) else None)&#10;        if 'categoria' in gb_copy.columns:&#10;            gb_copy['categoria'] = gb_copy['categoria'].apply(lambda x: ';'.join(uniq_preserve(listify(x))) if pd.notna(x) else None)&#10;&#10;        detail = pd.concat([gr_copy.reset_index(drop=True), gb_copy.reset_index(drop=True)], ignore_index=True, sort=False)&#10;        try:&#10;            detail['row_number'] = pd.to_numeric(detail['row_number'], errors='coerce').astype('Int64')&#10;        except Exception:&#10;            pass&#10;        if 'ts_ingest' not in detail.columns:&#10;            detail['ts_ingest'] = ts&#10;        return detail&#10;    except Exception as e:&#10;        raise RuntimeError(f&quot;_build_book_source_detail falló: {str(e)}&quot;)&#10;&#10;&#10;def _build_dim_from_merged(merged: pd.DataFrame) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;Construye la tabla dimensional a partir del DataFrame combinado (merged).&#10;&#10;    Selecciona los campos relevantes, elimina duplicados y aplica reglas de negocio para definir la fila ganadora&#10;    en caso de conflictos.&#10;    &quot;&quot;&quot;&#10;    if not isinstance(merged, pd.DataFrame):&#10;        return pd.DataFrame()&#10;    df = merged.copy()&#10;    base_fields = ['titulo','autor_principal','autores','editorial','anio_publicacion','fecha_publicacion','idioma','isbn10','isbn13','paginas','formato','categoria','precio','moneda']&#10;&#10;    if 'isbn13' in df.columns:&#10;        try:&#10;            df['isbn13_norm'] = df['isbn13'].apply(lambda v: try_normalize_isbn(v)[1] if pd.notna(v) and str(v).strip() != '' else None)&#10;            df['isbn13_valido'] = df['isbn13_norm'].apply(lambda v: bool(is_valid_isbn13(str(v))) if pd.notna(v) and str(v).strip() != '' else False)&#10;        except Exception:&#10;            df['isbn13_norm'] = df['isbn13']&#10;            df['isbn13_valido'] = False&#10;    else:&#10;        df['isbn13_norm'] = None&#10;        df['isbn13_valido'] = False&#10;&#10;    def _make_key(row):&#10;        if row.get('isbn13_valido'):&#10;            return str(row.get('isbn13_norm'))&#10;        return _canonical_key(row.get('titulo'), row.get('autor_principal'), row.get('editorial'), row.get('anio_publicacion'))&#10;&#10;    df['canonical_key'] = df.apply(lambda r: _make_key(r), axis=1)&#10;&#10;    def _completeness_score(row):&#10;        keys = ['titulo','autor_principal','editorial','fecha_publicacion','autores','precio','moneda']&#10;        score = sum(1 for k in keys if pd.notna(row.get(k)) and str(row.get(k)).strip() != '')&#10;        if row.get('isbn13_valido'):&#10;            score += 2&#10;        try:&#10;            gs = float(row.get('gb_match_score')) if row.get('gb_match_score') is not None and str(row.get('gb_match_score')).strip() != '' else 0.0&#10;            score += gs&#10;        except Exception:&#10;            pass&#10;        return float(score)&#10;&#10;    df['_completeness'] = df.apply(lambda r: _completeness_score(r), axis=1)&#10;&#10;    survivors = []&#10;    for key, group in df.groupby('canonical_key', dropna=False):&#10;        try:&#10;            if 'gb_match_score' in group.columns:&#10;                group['_gb_ms'] = pd.to_numeric(group['gb_match_score'], errors='coerce').fillna(0.0)&#10;            else:&#10;                group['_gb_ms'] = 0.0&#10;        except Exception:&#10;            group['_gb_ms'] = 0.0&#10;        group_sorted = group.sort_values(by=['_completeness','_gb_ms'], ascending=[False, False])&#10;        winner = group_sorted.iloc[0].copy()&#10;        try:&#10;            merged_autores = None&#10;            merged_categoria = None&#10;            for _, r in group.iterrows():&#10;                merged_autores = _merge_list_fields(merged_autores, r.get('autores'))&#10;                merged_categoria = _merge_list_fields(merged_categoria, r.get('categoria'))&#10;            winner['autores'] = merged_autores&#10;            winner['categoria'] = merged_categoria&#10;        except Exception:&#10;            pass&#10;        try:&#10;            fg = None&#10;            if pd.notna(winner.get('editorial')) and pd.notna(winner.get('fecha_publicacion')):&#10;                fg = winner.get('source_name') or 'merged'&#10;            else:&#10;                fg = winner.get('source_name') or 'merged'&#10;            winner['fuente_ganadora'] = fg&#10;        except Exception:&#10;            winner['fuente_ganadora'] = winner.get('source_name') or 'merged'&#10;        try:&#10;            prov = _compute_provenance(winner, base_fields)&#10;            winner['provenance'] = prov&#10;        except Exception:&#10;            winner['provenance'] = json.dumps({f: None for f in base_fields}, ensure_ascii=False)&#10;        survivors.append(winner)&#10;&#10;    if survivors:&#10;        dim = pd.DataFrame(survivors)&#10;    else:&#10;        dim = pd.DataFrame()&#10;&#10;    for c in ['_completeness','_gb_ms','isbn13_norm']:&#10;        if c in dim.columns:&#10;            try:&#10;                del dim[c]&#10;            except Exception:&#10;                pass&#10;    desired = ['canonical_key','isbn13','isbn10','titulo','titulo_normalizado','autor_principal','autores','editorial','anio_publicacion','fecha_publicacion','fecha_publicacion_parcial','fecha_publicacion_valida','idioma','idioma_valido','paginas','formato','categoria','precio','moneda','moneda_valida','fuente_ganadora','provenance','ts_ultima_actualizacion']&#10;    for d in desired:&#10;        if d not in dim.columns:&#10;            dim[d] = None&#10;    cols_ordered = [c for c in desired if c in dim.columns] + [c for c in dim.columns if c not in desired]&#10;    try:&#10;        dim = dim[cols_ordered]&#10;    except Exception:&#10;        pass&#10;    return dim&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Integración de Goodreads (JSON) y Google Books (CSV) a artefactos estándar.&#10;Produce standard/dim_book.parquet, standard/book_source_detail.parquet y docs/quality_metrics.json&#10;Mejoras implementadas:&#10;- Merge robusto por isbn13 y por match_key (titulo+autor) sin producir producto cartesiano&#10;- Aserciones configurables: unicidad book_id (bloqueante), porcentaje mínimo de títulos no nulos (por defecto 90%)&#10;- Fail-soft configurado: registros con errores se marcan en book_source_detail y se excluyen de dim_book&#10;- Resumen de aserciones incluido en docs/quality_metrics.json&#10;&quot;&quot;&quot;&#10;from __future__ import annotations&#10;&#10;import hashlib&#10;import json&#10;import logging&#10;from datetime import datetime, timezone&#10;from pathlib import Path&#10;from typing import Optional, Tuple, Dict, List, Mapping&#10;&#10;import pandas as pd&#10;import traceback&#10;import warnings&#10;from difflib import SequenceMatcher&#10;&#10;# Suprimir FutureWarnings ruidosos de pandas que no afectan la lógica actual&#10;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning, message=&quot;.*downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated.*&quot;)&#10;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning, message=&quot;.*DataFrameGroupBy.apply operated on the grouping columns.*&quot;)&#10;&#10;# is_valid_isbn13 se usa en otros módulos; no es necesario importarlo aquí para evitar warning&#10;# from utils_isbn import is_valid_isbn13&#10;from utils_quality import (&#10;    compute_quality_metrics,&#10;    listify,&#10;    normalize_language,&#10;    normalize_whitespace,&#10;    parse_date_to_iso,&#10;    uniq_preserve,&#10;    validate_currency,&#10;    validate_language,&#10;    normalize_currency, nulls_by_column,&#10;)&#10;# Añadir import de utils_isbn necesario&#10;from utils_isbn import try_normalize_isbn, is_valid_isbn13&#10;&#10;# Evitar FutureWarning sobre downcasting en operaciones futuras de pandas&#10;try:&#10;    pd.set_option('future.no_silent_downcasting', True)&#10;except Exception:&#10;    pass&#10;&#10;# Funciones auxiliares para sanitizar registros antes de calcular métricas&#10;def _sanitize_value(v):&#10;    try:&#10;        if v is None:&#10;            return None&#10;        if isinstance(v, float) and pd.isna(v):&#10;            return None&#10;        if isinstance(v, (bytes, bytearray)):&#10;            try:&#10;                return v.decode('utf-8')&#10;            except Exception:&#10;                return str(v)&#10;        if isinstance(v, (str, bool, int, float)):&#10;            return v&#10;        if isinstance(v, (list, dict)):&#10;            try:&#10;                return json.dumps(v, ensure_ascii=False)&#10;            except Exception:&#10;                return str(v)&#10;        if hasattr(v, 'tolist'):&#10;            try:&#10;                return v.tolist()&#10;            except Exception:&#10;                pass&#10;        return str(v)&#10;    except Exception:&#10;        return None&#10;&#10;&#10;def _sanitize_records_for_metrics(records: List[Mapping[str, object]]) -&gt; List[Mapping[str, object]]:&#10;    sanitized: List[Dict[str, object]] = []&#10;    for r in records:&#10;        if not isinstance(r, Mapping):&#10;            sanitized.append({})&#10;            continue&#10;        row: Dict[str, object] = {}&#10;        for k, v in r.items():&#10;            try:&#10;                row[k] = _sanitize_value(v)&#10;            except Exception:&#10;                row[k] = None&#10;        sanitized.append(row)&#10;    return sanitized&#10;&#10;&#10;# Helpers seguros para leer y escribir en DataFrames con índices no estándar&#10;def _safe_set(df: pd.DataFrame, idx, col: str, val) -&gt; bool:&#10;    try:&#10;        if idx in df.index:&#10;            df.loc[idx, col] = val&#10;            return True&#10;    except Exception:&#10;        pass&#10;    try:&#10;        pos = int(idx)&#10;        df.iloc[pos, df.columns.get_loc(col)] = val&#10;        return True&#10;    except Exception:&#10;        pass&#10;    try:&#10;        df.at[idx, col] = val&#10;        return True&#10;    except Exception:&#10;        return False&#10;&#10;&#10;def _safe_get(df: pd.DataFrame, idx, col: str):&#10;    try:&#10;        if idx in df.index:&#10;            return df.loc[idx, col]&#10;    except Exception:&#10;        pass&#10;    try:&#10;        pos = int(idx)&#10;        return df.iloc[pos][col]&#10;    except Exception:&#10;        pass&#10;    try:&#10;        return df.at[idx, col]&#10;    except Exception:&#10;        return None&#10;&#10;# Import robusto de funciones de logging en work/&#10;try:&#10;    from work.utils_logging import log_rule_jsonl, write_run_summary, ensure_work_dirs&#10;except Exception:&#10;    def log_rule_jsonl(*args, **kwargs):&#10;        return None&#10;    def write_run_summary(*args, **kwargs):&#10;        return None&#10;    def ensure_work_dirs(*args, **kwargs):&#10;        return None&#10;&#10;# Configuración y paths&#10;ROOT = Path(__file__).resolve().parents[1]&#10;LANDING = ROOT / &quot;landing&quot;&#10;STANDARD = ROOT / &quot;standard&quot;&#10;DOCS = ROOT / &quot;docs&quot;&#10;&#10;# Aserciones / umbrales para la rúbrica&#10;ASSERT_UNIQUENESS_BOOK_ID = True  # si True, bloquear si existen duplicados irreconciliables&#10;MIN_TITLES_PCT = 0.90  # mínimo % de títulos no nulos requerido&#10;# Umbrales para matching con candidatos de Google Books&#10;# umbral reducido para aceptar candidatos más amplios y mejorar cobertura de ISBN/idioma/fecha&#10;GB_MATCH_THRESHOLD = 20.0  # puntuación mínima para aceptar candidato de Google Books cuando Goodreads no tiene ISBN&#10;# Umbral para matching difuso entre títulos/autores (0..1). Ajustable.&#10;GB_FUZZY_THRESHOLD = 0.75&#10;&#10;# Logger sencillo&#10;logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')&#10;logger = logging.getLogger(__name__)&#10;&#10;# Mapeo sencillo de símbolos a ISO para monedas&#10;SYMBOL_TO_ISO = {&#10;    &quot;$&quot;: &quot;USD&quot;,&#10;    &quot;\u20ac&quot;: &quot;EUR&quot;,&#10;    &quot;\u00a3&quot;: &quot;GBP&quot;,&#10;    &quot;\u00a5&quot;: &quot;JPY&quot;,&#10;    &quot;R$&quot;: &quot;BRL&quot;,&#10;}&#10;&#10;&#10;def _canonical_key(title: str, author: str, publisher: Optional[str], year: Optional[int]) -&gt; str:&#10;    # Genera clave SHA1 estable para fallback cuando isbn13 no existe&#10;    def _norm(x: Optional[object]) -&gt; str:&#10;        if x is None or (isinstance(x, float) and pd.isna(x)):&#10;            return &quot;&quot;&#10;        s = str(x)&#10;        return s.strip().lower()&#10;&#10;    base = &quot;|&quot;.join([&#10;        _norm(title),&#10;        _norm(author),&#10;        _norm(publisher),&#10;        _norm(year),&#10;    ])&#10;    return hashlib.sha1(base.encode(&quot;utf-8&quot;)).hexdigest()&#10;&#10;&#10;def _load_sources() -&gt; Tuple[pd.DataFrame, pd.DataFrame]:&#10;    &quot;&quot;&quot;Carga archivos de landing asegurando ISBN como string para evitar coerciones a int.&#10;&#10;    Comentarios:&#10;    - Supuesto: los ficheros en `landing/` son la &quot;fuente de la verdad&quot; y no deben ser sobrescritos por integración.&#10;    - Se fuerza dtype string para isbn para evitar conversión automática a int/float que rompe pyarrow.&#10;    KEYWORDS: LOAD_SOURCES, COERCE_ISBN&#10;    &quot;&quot;&quot;&#10;    gr_path = LANDING / &quot;goodreads_books.json&quot;&#10;    gb_path = LANDING / &quot;googlebooks_books.csv&quot;&#10;&#10;    if not gr_path.exists():&#10;        raise FileNotFoundError(f&quot;Falta archivo de landing esperado: {gr_path}&quot;)&#10;    if not gb_path.exists():&#10;        raise FileNotFoundError(f&quot;Falta archivo de landing esperado: {gb_path}&quot;)&#10;&#10;    with open(gr_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        gr_json = json.load(f)&#10;    gr = pd.DataFrame(gr_json.get(&quot;records&quot;, []))&#10;    gr = gr.copy()&#10;    gr[&quot;source_name&quot;] = &quot;goodreads&quot;&#10;    gr[&quot;source_file&quot;] = str(gr_path.name)&#10;&#10;    gb = pd.read_csv(gb_path, dtype={&quot;isbn13&quot;: &quot;string&quot;, &quot;isbn10&quot;: &quot;string&quot;})&#10;    gb = gb.copy()&#10;    gb[&quot;source_name&quot;] = &quot;google_books&quot;&#10;    gb[&quot;source_file&quot;] = str(gb_path.name)&#10;&#10;    # --- nuevo: exponer numero de fila CSV para poder mapear con googlebooks_candidates.csv ---&#10;    try:&#10;        import numpy as _np&#10;        gb['_csv_row'] = _np.arange(1, len(gb) + 1)&#10;    except:&#10;        gb['_csv_row'] = list(range(1, len(gb) + 1))&#10;&#10;    # Nota: anteriormente aquí se intentaba usar `gb_price_map` que no existe en este&#10;    # contexto (provoca NameError). La lógica de mapeo de precios se realiza más abajo&#10;    # dentro de `_merge_sources` a partir de `googlebooks_candidates.json` y, por tanto,&#10;    # no debe ejecutarse ni referenciarse aquí.&#10;&#10;    return gr, gb&#10;&#10;&#10;def _normalize_frames(gr: pd.DataFrame, gb: pd.DataFrame) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:&#10;    # Normaliza columnas y formatos básicos en ambos dataframes&#10;    gr = gr.copy()&#10;    gr.rename(columns={&#10;        &quot;title&quot;: &quot;titulo&quot;,&#10;        &quot;author&quot;: &quot;autor_principal&quot;,&#10;        &quot;isbn13&quot;: &quot;isbn13&quot;,&#10;        &quot;isbn10&quot;: &quot;isbn10&quot;,&#10;    }, inplace=True)&#10;    for c in (&quot;isbn13&quot;, &quot;isbn10&quot;):&#10;        if c in gr.columns:&#10;            gr[c] = gr[c].apply(lambda v: str(v).strip() if pd.notna(v) and str(v).strip() != &quot;&quot; else None)&#10;    gr[&quot;idioma&quot;] = None&#10;    gr[&quot;categoria&quot;] = None&#10;    gr[&quot;fecha_publicacion&quot;] = None&#10;    gr[&quot;precio&quot;] = None&#10;    gr[&quot;moneda&quot;] = None&#10;&#10;    gb = gb.copy()&#10;    gb.rename(columns={&#10;        &quot;title&quot;: &quot;titulo&quot;,&#10;        &quot;authors&quot;: &quot;autores&quot;,&#10;        &quot;publisher&quot;: &quot;editorial&quot;,&#10;        &quot;pub_date&quot;: &quot;fecha_publicacion&quot;,&#10;        &quot;language&quot;: &quot;idioma&quot;,&#10;        &quot;categories&quot;: &quot;categoria&quot;,&#10;        &quot;price_amount&quot;: &quot;precio&quot;,&#10;        &quot;price_currency&quot;: &quot;moneda&quot;,&#10;    }, inplace=True)&#10;    for c in (&quot;isbn13&quot;, &quot;isbn10&quot;):&#10;        if c in gb.columns:&#10;            gb[c] = gb[c].apply(lambda v: str(v).strip() if pd.notna(v) and str(v).strip() != &quot;&quot; else None)&#10;&#10;    # autores/categoria como listas serializadas&#10;    if &quot;autores&quot; in gb.columns:&#10;        gb[&quot;autores&quot;] = gb[&quot;autores&quot;].apply(lambda x: &quot;;&quot;.join(uniq_preserve(listify(x))) if pd.notna(x) else None)&#10;        gb[&quot;autor_principal&quot;] = gb[&quot;autores&quot;].apply(lambda x: str(x).split(&quot;;&quot;)[0] if pd.notna(x) and str(x).strip() != &quot;&quot; else None)&#10;    else:&#10;        gb[&quot;autor_principal&quot;] = None&#10;    if &quot;categoria&quot; in gb.columns:&#10;        gb[&quot;categoria&quot;] = gb[&quot;categoria&quot;].apply(lambda x: &quot;;&quot;.join(uniq_preserve(listify(x))) if pd.notna(x) else None)&#10;&#10;    if &quot;idioma&quot; in gb.columns:&#10;        gb[&quot;idioma&quot;] = gb[&quot;idioma&quot;].apply(lambda x: normalize_language(x) if pd.notna(x) else None)&#10;&#10;    gb[&quot;_match_title&quot;] = gb[&quot;titulo&quot;].apply(lambda s: normalize_whitespace(str(s).lower()) if pd.notna(s) else None)&#10;    gb[&quot;_match_author&quot;] = gb[&quot;autor_principal&quot;].apply(lambda s: normalize_whitespace(str(s).lower()) if pd.notna(s) else None)&#10;&#10;    # Para Goodreads crear keys similares para emparejar&#10;    gr[&quot;_match_title&quot;] = gr.apply(lambda r: normalize_whitespace(str(r.get(&quot;titulo&quot;) or &quot;&quot;).lower()), axis=1)&#10;    gr[&quot;_match_author&quot;] = gr.apply(lambda r: normalize_whitespace(str(r.get(&quot;autor_principal&quot;) or &quot;&quot;).lower()), axis=1)&#10;    gr[&quot;_match_key&quot;] = gr.apply(lambda r: (f&quot;{r.get('_match_title')}|{r.get('_match_author')}&quot; if r.get('_match_title') or r.get('_match_author') else None), axis=1)&#10;    gb[&quot;_match_key&quot;] = gb.apply(lambda r: (f&quot;{r.get('_match_title') or ''}|{r.get('_match_author') or ''}&quot; if (r.get('_match_title') or r.get('_match_author')) else None), axis=1)&#10;&#10;    return gr, gb&#10;&#10;&#10;def _merge_sources(gr: pd.DataFrame, gb: pd.DataFrame) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;Merge robusto por isbn13 (primario) y por _match_key (secundario) sin producir producto cartesiano.&#10;    Devuelve DataFrame con columnas limpias y sufijos explícitos para selección posterior.&#10;    &quot;&quot;&quot;&#10;    # Intentar asignar ISBN a registros de Goodreads sin isbn usando candidatos de googlebooks_candidates.json&#10;    candidates_path = LANDING / &quot;googlebooks_candidates.json&quot;&#10;    gb_cands_map: Dict[str, Tuple[Optional[str], Optional[float]]] = {}&#10;    gb_price_map: Dict[str, Tuple[Optional[float], Optional[str]]] = {}&#10;    if candidates_path.exists():&#10;        try:&#10;            with open(candidates_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;                cand_list = json.load(f)&#10;            for c in cand_list:&#10;                rec_id = c.get(&quot;rec_id&quot;)&#10;                idx = c.get(&quot;input_index&quot;)&#10;                best_score = c.get(&quot;best_score&quot;)&#10;                chosen_isbn = None&#10;                # elegir el candidate con mayor score que tenga isbn13&#10;                for cand in (c.get(&quot;candidates&quot;) or []):&#10;                    if cand.get(&quot;isbn13&quot;) and (best_score is None or float(cand.get(&quot;score&quot;, 0)) &gt;= float(best_score) - 0.0001):&#10;                        chosen_isbn = cand.get(&quot;isbn13&quot;)&#10;                        break&#10;                # fallback: tomar el candidato con mayor score que tenga isbn13&#10;                if not chosen_isbn:&#10;                    best_c = None&#10;                    best_s = -1.0&#10;                    for cand in (c.get(&quot;candidates&quot;) or []):&#10;                        s = float(cand.get(&quot;score&quot;)) if cand.get(&quot;score&quot;) is not None else 0.0&#10;                        if cand.get(&quot;isbn13&quot;) and s &gt; best_s:&#10;                            best_s = s&#10;                            best_c = cand&#10;                    if best_c:&#10;                        chosen_isbn = best_c.get(&quot;isbn13&quot;)&#10;                        best_score = best_s&#10;                if rec_id:&#10;                    gb_cands_map[f&quot;rec:{rec_id}&quot;] = (chosen_isbn, best_score)&#10;                if idx is not None:&#10;                    gb_cands_map[f&quot;idx:{idx}&quot;] = (chosen_isbn, best_score)&#10;                # title-author key&#10;                t = c.get(&quot;title&quot;) or &quot;&quot;&#10;                a = c.get(&quot;author&quot;) or &quot;&quot;&#10;                gb_cands_map[f&quot;ta:{t.strip()}|{a.strip()}&quot;] = (chosen_isbn, best_score)&#10;&#10;                # --- nuevo: mapear precio/moneda desde candidatos para mejorar cobertura ---&#10;                # buscar any candidate with currency&#10;                price_amt = None&#10;                price_cur = None&#10;                # iterar TODOS los candidatos y tomar el primero con currency si existe&#10;                for cand in (c.get('candidates') or []):&#10;                    if cand.get('price_currency') and cand.get('price_currency') not in (None, '', pd.NA):&#10;                        price_amt = cand.get('price_amount')&#10;                        price_cur = cand.get('price_currency')&#10;                        break&#10;                # asignar a varias keys (rec, idx y ta y row:{csv_row_number}) para lookup posterior&#10;                if rec_id:&#10;                    gb_price_map[f&quot;rec:{rec_id}&quot;] = (price_amt, price_cur)&#10;                if idx is not None:&#10;                    gb_price_map[f&quot;idx:{idx}&quot;] = (price_amt, price_cur)&#10;                csv_row = c.get('csv_row_number')&#10;                if csv_row is not None:&#10;                    gb_price_map[f&quot;row:{csv_row}&quot;] = (price_amt, price_cur)&#10;                # crear clave normalizada para title|author&#10;                t_key = normalize_whitespace(t).lower() if t else ''&#10;                a_key = normalize_whitespace(a).lower() if a else ''&#10;                gb_price_map[f&quot;ta:{t_key}|{a_key}&quot;] = (price_amt, price_cur)&#10;        except Exception:&#10;            gb_cands_map = {}&#10;            gb_price_map = {}&#10;&#10;    # Aplicar precios/monedas detectados en gb_price_map a filas de 'gb' cuando sea posible.&#10;    # Soporta claves 'row:' y claves normalizadas 'ta:' usando gb._match_key&#10;    try:&#10;        if gb_price_map and isinstance(gb, pd.DataFrame):&#10;            for key, (pamt, pcur) in gb_price_map.items():&#10;                if key.startswith('row:'):&#10;                    try:&#10;                        part = key.split(':', 1)[1]&#10;                        rownum = int(part) if (part is not None and str(part).isdigit()) else None&#10;                        mask = gb.get('_csv_row') == rownum&#10;                        if 'precio' in gb.columns and pamt is not None:&#10;                            gb.loc[mask &amp; gb['precio'].isna(), 'precio'] = pamt&#10;                        if 'moneda' in gb.columns and pcur is not None:&#10;                            gb.loc[mask &amp; gb['moneda'].isna(), 'moneda'] = pcur&#10;                    except Exception:&#10;                        continue&#10;                elif key.startswith('ta:'):&#10;                    try:&#10;                        # key form: ta:normalized_title|normalized_author&#10;                        _, ta = key.split(':', 1)&#10;                        tpart, apart = ta.split('|', 1)&#10;                        # usar _match_key creado en _normalize_frames para comparar&#10;                        match_key = f&quot;{tpart}|{apart}&quot;&#10;                        mask = gb.get('_match_key') == match_key&#10;                        if 'precio' in gb.columns and pamt is not None:&#10;                            gb.loc[mask &amp; gb['precio'].isna(), 'precio'] = pamt&#10;                        if 'moneda' in gb.columns and pcur is not None:&#10;                            gb.loc[mask &amp; gb['moneda'].isna(), 'moneda'] = pcur&#10;                    except Exception:&#10;                        continue&#10;    except Exception:&#10;        # no bloquear si no se puede aplicar el mapeo&#10;        pass&#10;&#10;    # Dividir gr en con isbn y sin isbn&#10;    gr_with_isbn = gr[gr[&quot;isbn13&quot;].notna()].copy()&#10;    gr_no_isbn = gr[gr[&quot;isbn13&quot;].isna()].copy()&#10;&#10;    # Asignar isbn13 desde candidatos cuando el score supera umbral&#10;    def _assign_isbn_from_candidates(row):&#10;        # buscar por rec_id (book_url), index o title|author&#10;        rec = row.get(&quot;book_url&quot;) or row.get(&quot;id&quot;)&#10;        keys = []&#10;        if rec:&#10;            keys.append(f&quot;rec:{rec}&quot;)&#10;        # index-based key (input index)&#10;        if row.name is not None:&#10;            try:&#10;                safe_idx = None&#10;                try:&#10;                    if isinstance(row.name, (int, float)):&#10;                        safe_idx = int(row.name)&#10;                    else:&#10;                        rs = str(row.name).strip()&#10;                        if rs.isdigit():&#10;                            safe_idx = int(rs)&#10;                except Exception:&#10;                    safe_idx = None&#10;                if safe_idx is not None:&#10;                    keys.append(f&quot;idx:{safe_idx}&quot;)&#10;            except Exception:&#10;                pass&#10;        # intentar varias formas de title|author: original fields y campos normalizados&#10;        ta_candidates = []&#10;        ta_candidates.append(f&quot;ta:{(row.get('title') or '').strip()}|{(row.get('author') or '').strip()}&quot;)&#10;        ta_candidates.append(f&quot;ta:{(row.get('titulo') or row.get('title') or '').strip()}|{(row.get('autor_principal') or row.get('author') or '').strip()}&quot;)&#10;        # también intentar usar _match_key si está presente (normalizado título|autor)&#10;        mk = None&#10;        if '_match_key' in row.index:&#10;            mk = row.get('_match_key')&#10;        elif row.get('_match_title') is not None or row.get('_match_author') is not None:&#10;            mk = f&quot;{row.get('_match_title') or ''}|{row.get('_match_author') or ''}&quot;&#10;        if mk:&#10;            keys.append(f&quot;ta:{mk}&quot;)&#10;        # añadir ta candidates&#10;        keys.extend(ta_candidates)&#10;&#10;        for k in keys:&#10;            if not k:&#10;                continue&#10;            if k in gb_cands_map:&#10;                isbn, score = gb_cands_map[k]&#10;                try:&#10;                    if isbn and score is not None and float(score) &gt;= GB_MATCH_THRESHOLD:&#10;                        # Registrar regla: asignación de ISBN desde candidatos&#10;                        try:&#10;                            log_rule_jsonl({&#10;                                'rule': 'assign_isbn_from_candidates',&#10;                                'key': k,&#10;                                'assigned_isbn': isbn,&#10;                                'score': float(score),&#10;                                'rec': rec,&#10;                            })&#10;                        except Exception:&#10;                            pass&#10;                        return isbn, float(score)&#10;                except Exception:&#10;                    continue&#10;        return None, None&#10;&#10;    if not gr_no_isbn.empty and gb_cands_map:&#10;        assigned = gr_no_isbn.apply(lambda r: _assign_isbn_from_candidates(r), axis=1, result_type='expand')&#10;        # manejar distintos tipos de retorno de apply: DataFrame, Series o ndarray&#10;        try:&#10;            if isinstance(assigned, pd.DataFrame) and assigned.shape[1] &gt;= 1:&#10;                gr_no_isbn['isbn13_assigned'] = assigned.iloc[:, 0]&#10;                if assigned.shape[1] &gt; 1:&#10;                    gr_no_isbn['gb_assigned_score'] = assigned.iloc[:, 1]&#10;            elif isinstance(assigned, pd.Series):&#10;                # cada elemento puede ser tuple/list -&gt; expandir con seguridad&#10;                gr_no_isbn['isbn13_assigned'] = assigned.apply(lambda v: v[0] if isinstance(v, (list, tuple)) and len(v) &gt; 0 else None)&#10;                gr_no_isbn['gb_assigned_score'] = assigned.apply(lambda v: v[1] if isinstance(v, (list, tuple)) and len(v) &gt; 1 else None)&#10;            else:&#10;                # intentar convertir a array y extraer columnas si posible&#10;                try:&#10;                    arr = pd.DataFrame(list(assigned))&#10;                    if arr.shape[1] &gt;= 1:&#10;                        gr_no_isbn['isbn13_assigned'] = arr.iloc[:, 0].values&#10;                    if arr.shape[1] &gt; 1:&#10;                        gr_no_isbn['gb_assigned_score'] = arr.iloc[:, 1].values&#10;                except Exception:&#10;                    pass&#10;        except Exception:&#10;            # no bloquear pipeline por fallo al interpretar 'assigned'&#10;            pass&#10;        # promover asignación a isbn13 si existe&#10;        try:&#10;            if 'isbn13_assigned' in gr_no_isbn.columns:&#10;                gr_no_isbn.loc[gr_no_isbn['isbn13_assigned'].notna(), 'isbn13'] = gr_no_isbn.loc[gr_no_isbn['isbn13_assigned'].notna(), 'isbn13_assigned']&#10;                gr_no_isbn.drop(columns=['isbn13_assigned'], inplace=True, errors='ignore')&#10;        except Exception:&#10;            pass&#10;&#10;    # --- Fallback difuso: si tras candidates aún hay registros sin isbn, hacer fuzzy match contra GB ---&#10;    try:&#10;        # preparar comparación: usar _match_title/_match_author normalizados en gb&#10;        if not gr_no_isbn.empty and isinstance(gb, pd.DataFrame):&#10;            # garantizar columnas de normalización en gb&#10;            if '_match_title' not in gb.columns:&#10;                gb['_match_title'] = gb['titulo'].apply(lambda s: normalize_whitespace(str(s).lower()) if pd.notna(s) else '')&#10;            if '_match_author' not in gb.columns:&#10;                gb['_match_author'] = gb['autor_principal'].apply(lambda s: normalize_whitespace(str(s).lower()) if pd.notna(s) else '')&#10;&#10;            def _score_match(gr_title, gr_author, gb_title, gb_author, gb_isbn_present):&#10;                # title and author are normalized strings&#10;                try:&#10;                    title_ratio = SequenceMatcher(None, gr_title, gb_title).ratio() if gr_title and gb_title else 0.0&#10;                except Exception:&#10;                    title_ratio = 0.0&#10;                try:&#10;                    author_ratio = SequenceMatcher(None, gr_author, gb_author).ratio() if gr_author and gb_author else 0.0&#10;                except Exception:&#10;                    author_ratio = 0.0&#10;                # token overlap on titles&#10;                try:&#10;                    tset_g = set([w for w in gr_title.split() if w])&#10;                    tset_b = set([w for w in gb_title.split() if w])&#10;                    token_overlap = (len(tset_g &amp; tset_b) / max(len(tset_g | tset_b), 1)) if tset_g or tset_b else 0.0&#10;                except Exception:&#10;                    token_overlap = 0.0&#10;                score = 0.6 * title_ratio + 0.3 * author_ratio + 0.1 * token_overlap&#10;                # dar leve bonus si GB row tiene isbn&#10;                if gb_isbn_present:&#10;                    score += 0.05&#10;                return float(score)&#10;&#10;            fuzzy_results = []&#10;            for idx, gr_row in gr_no_isbn.iterrows():&#10;                if pd.notna(gr_row.get('isbn13')) and str(gr_row.get('isbn13')).strip() != '':&#10;                    continue&#10;                gr_title_norm = normalize_whitespace(str(gr_row.get('titulo') or gr_row.get('title') or '').lower())&#10;                gr_author_norm = normalize_whitespace(str(gr_row.get('autor_principal') or gr_row.get('author') or '').lower())&#10;                best_score = -1.0&#10;                best_isbn = None&#10;                best_gb_index = None&#10;                # iterar gb y puntuar&#10;                for gb_i, gb_row in gb.iterrows():&#10;                    gb_title = str(gb_row.get('_match_title') or '').lower()&#10;                    gb_author = str(gb_row.get('_match_author') or '').lower()&#10;                    gb_has_isbn = bool(gb_row.get('isbn13'))&#10;                    s = _score_match(gr_title_norm, gr_author_norm, gb_title, gb_author, gb_has_isbn)&#10;                    if s &gt; best_score:&#10;                        best_score = s&#10;                        best_isbn = gb_row.get('isbn13')&#10;                        best_gb_index = gb_i&#10;                # si mejor score supera umbral, asignar&#10;                try:&#10;                    if best_score &gt;= GB_FUZZY_THRESHOLD and best_isbn:&#10;                        # registrar regla&#10;                        try:&#10;                            gi = int(idx) if isinstance(idx, (int, float)) else (int(str(idx)) if isinstance(idx, str) and str(idx).isdigit() else str(idx))&#10;                            gbi = int(best_gb_index) if (best_gb_index is not None and (isinstance(best_gb_index, (int, float)) or (isinstance(best_gb_index, str) and str(best_gb_index).isdigit()))) else best_gb_index&#10;                            log_rule_jsonl({'rule': 'fuzzy_assign_isbn', 'gr_index': gi, 'assigned_isbn': best_isbn, 'score': float(best_score), 'gb_index': gbi})&#10;                        except Exception:&#10;                            pass&#10;                        _safe_set(gr_no_isbn, idx, 'isbn13_assigned', best_isbn)&#10;                        _safe_set(gr_no_isbn, idx, 'gb_assigned_score', float(best_score))&#10;                except Exception:&#10;                    continue&#10;    except Exception:&#10;        # no bloquear pipeline si fuzzy fallback falla&#10;        pass&#10;&#10;    logger.info(&quot;gr_total=%d gr_with_isbn=%d gr_no_isbn=%d gb_total=%d&quot;, len(gr), len(gr_with_isbn), len(gr_no_isbn), len(gb))&#10;&#10;    # Merge por isbn13 para todos los registros que tienen isbn13 en gr&#10;    merged_by_isbn = pd.merge(&#10;        gr_with_isbn.add_suffix(&quot;_gr&quot;),&#10;        gb.add_suffix(&quot;_gb&quot;),&#10;        left_on=&quot;isbn13_gr&quot;,&#10;        right_on=&quot;isbn13_gb&quot;,&#10;        how=&quot;left&quot;,&#10;        suffixes=(&quot;_gr&quot;, &quot;_gb&quot;),&#10;    )&#10;&#10;    # Merge por match_key para registros sin isbn (título+autor normalizados)&#10;    merged_by_key = pd.merge(&#10;        gr_no_isbn.add_suffix(&quot;_gr&quot;),&#10;        gb.add_suffix(&quot;_gb&quot;),&#10;        left_on=&quot;_match_key_gr&quot;,&#10;        right_on=&quot;_match_key_gb&quot;,&#10;        how=&quot;left&quot;,&#10;        suffixes=(&quot;_gr&quot;, &quot;_gb&quot;),&#10;    )&#10;&#10;    # Identificar gb rows que no han sido emparejadas por isbn ni por key (gb-only)&#10;    # Mejor detección de filas de GB ya emparejadas: usar _csv_row si está disponible&#10;    matched_gb_rows = set()&#10;    matched_gb_isbns = set()&#10;    try:&#10;        if '_csv_row_gb' in merged_by_isbn.columns:&#10;            matched_gb_rows.update(merged_by_isbn['_csv_row_gb'].dropna().astype(int).astype(str).unique().tolist())&#10;        if '_csv_row_gb' in merged_by_key.columns:&#10;            matched_gb_rows.update(merged_by_key['_csv_row_gb'].dropna().astype(int).astype(str).unique().tolist())&#10;    except Exception:&#10;        # fallback a ISBN si no podemos usar _csv_row&#10;        try:&#10;            matched_gb_isbns.update(merged_by_isbn['isbn13_gb'].dropna().unique().tolist())&#10;            matched_gb_isbns.update(merged_by_key['isbn13_gb'].dropna().unique().tolist())&#10;        except Exception:&#10;            matched_gb_isbns = set()&#10;&#10;    if matched_gb_rows:&#10;        # gb['_csv_row'] fue creado en _load_sources; comparar como strings/números&#10;        try:&#10;            gb_only_mask = ~gb['_csv_row'].astype(str).isin(matched_gb_rows)&#10;        except Exception:&#10;            gb_only_mask = ~gb['_csv_row'].isin([int(x) for x in matched_gb_rows if str(x).isdigit()])&#10;    else:&#10;        # uso de ISBN como fallback&#10;        try:&#10;            gb_only_mask = ~gb['isbn13'].isin([v for v in matched_gb_isbns if v is not None])&#10;        except Exception:&#10;            gb_only_mask = pd.Series([True] * len(gb), index=gb.index)&#10;&#10;    gb_only = gb[gb_only_mask].copy()&#10;&#10;    # --- diagnóstico agregado de matches ---&#10;    try:&#10;        logger.info('gb rows total=%d; gb_only after filtering=%d; gr_with_isbn=%d; gr_no_isbn=%d', len(gb), len(gb_only), len(gr_with_isbn), len(gr_no_isbn))&#10;    except Exception:&#10;        pass&#10;&#10;    # Normalizar estructuras para concatenar: queremos columnas con sufijos *_gr y *_gb&#10;    def _ensure_columns(df: pd.DataFrame, cols: List[str]) -&gt; pd.DataFrame:&#10;        for c in cols:&#10;            if c not in df.columns:&#10;                df[c] = None&#10;        return df&#10;&#10;    # Determinar conjunto de columnas esperadas (sufijos)&#10;    cols_gr = [c for c in merged_by_isbn.columns if c.endswith(&quot;_gr&quot;)] + [c for c in merged_by_key.columns if c.endswith(&quot;_gr&quot;)]&#10;    cols_gb = [c for c in merged_by_isbn.columns if c.endswith(&quot;_gb&quot;)] + [c for c in merged_by_key.columns if c.endswith(&quot;_gb&quot;)]&#10;    cols_gr = list(dict.fromkeys(cols_gr))&#10;    cols_gb = list(dict.fromkeys(cols_gb))&#10;&#10;    # Preparar gb_only en formato con sufijo _gb&#10;    gb_only_prefixed = gb_only.copy()&#10;    gb_only_prefixed = gb_only_prefixed.add_suffix(&quot;_gb&quot;)&#10;    # marcar origen para facilitar filtrado posterior: gb_only&#10;    try:&#10;        gb_only_prefixed['_source_type'] = 'gb_only'&#10;    except Exception:&#10;        pass&#10;    # Añadir columnas _gr vacías para compatibilidad&#10;    for c in cols_gr:&#10;        if c not in gb_only_prefixed.columns:&#10;            gb_only_prefixed[c] = None&#10;    # Asegurar columnas esperadas en merged_by_isbn y merged_by_key&#10;    merged_by_isbn = _ensure_columns(merged_by_isbn, cols_gb + cols_gr)&#10;    merged_by_key = _ensure_columns(merged_by_key, cols_gb + cols_gr)&#10;    # marcar origen para merges basados en isbn/key&#10;    try:&#10;        merged_by_isbn['_source_type'] = 'merged_by_isbn'&#10;    except Exception:&#10;        pass&#10;    try:&#10;        merged_by_key['_source_type'] = 'merged_by_key'&#10;    except Exception:&#10;        pass&#10;&#10;    # Concatenar todas las filas en un único merged (alineado por columnas)&#10;    # Evitar FutureWarning: eliminar columnas que sean totalmente NA en cada DataFrame&#10;    def _drop_all_na_cols(df: pd.DataFrame) -&gt; pd.DataFrame:&#10;        try:&#10;            return df.dropna(axis=1, how='all')&#10;        except Exception:&#10;            return df&#10;&#10;    merged_by_isbn = _drop_all_na_cols(merged_by_isbn)&#10;    merged_by_key = _drop_all_na_cols(merged_by_key)&#10;    gb_only_prefixed = _drop_all_na_cols(gb_only_prefixed)&#10;&#10;    # Log de diagnóstico: tamaños intermedios antes de concatenar&#10;    try:&#10;        logger.info('Merged components sizes: by_isbn=%d by_key=%d gb_only=%d',&#10;                    0 if merged_by_isbn is None else (len(merged_by_isbn) if hasattr(merged_by_isbn, '__len__') else 0),&#10;                    0 if merged_by_key is None else (len(merged_by_key) if hasattr(merged_by_key, '__len__') else 0),&#10;                    0 if gb_only_prefixed is None else (len(gb_only_prefixed) if hasattr(gb_only_prefixed, '__len__') else 0))&#10;    except Exception:&#10;        pass&#10;&#10;    # Evitar concatenar DataFrames vacíos (reduce warnings y asegura dtypes cohertes)&#10;    to_concat = [df for df in (merged_by_isbn.reset_index(drop=True) if isinstance(merged_by_isbn, pd.DataFrame) else merged_by_isbn,&#10;                               merged_by_key.reset_index(drop=True) if isinstance(merged_by_key, pd.DataFrame) else merged_by_key,&#10;                               gb_only_prefixed.reset_index(drop=True) if isinstance(gb_only_prefixed, pd.DataFrame) else gb_only_prefixed)&#10;                if isinstance(df, pd.DataFrame) and df.shape[0] &gt; 0]&#10;    if to_concat:&#10;        # diagnóstico: comprobar suma de filas antes de concatenar&#10;        component_counts = []&#10;        try:&#10;            component_counts = [len(df) for df in to_concat]&#10;            sum_rows = sum(component_counts)&#10;            logger.info('Component row counts before concat: %s; sum=%d', component_counts, sum_rows)&#10;        except Exception:&#10;            sum_rows = None&#10;        merged = pd.concat(to_concat, ignore_index=True, sort=False)&#10;        # diagnóstico adicional: calcular expected a partir de componentes reales&#10;        try:&#10;            expected = (len(merged_by_isbn) if isinstance(merged_by_isbn, pd.DataFrame) else 0) + (len(merged_by_key) if isinstance(merged_by_key, pd.DataFrame) else 0) + (len(gb_only_prefixed) if isinstance(gb_only_prefixed, pd.DataFrame) else 0)&#10;            logger.info('Expected merged rows (by_isbn + by_key + gb_only) = %d; actual merged = %d', expected, len(merged))&#10;            if expected is not None and len(merged) != expected:&#10;                logger.warning('Concatenación: filas inesperadas. expected=%s actual=%d; component_counts=%s', str(expected), len(merged), component_counts)&#10;        except Exception:&#10;            logger.debug('No se pudo calcular expected rows con componentes')&#10;        try:&#10;             if sum_rows is not None and len(merged) != sum_rows:&#10;                 logger.warning('Concatenación resultó en diferente número de filas: expected_sum=%s actual=%d', str(sum_rows), len(merged))&#10;                 # mostrar primeros indices y muestras para diagnóstico&#10;                 try:&#10;                     for i, df in enumerate(to_concat):&#10;                         logger.debug('component %d head:\n%s', i, df.head(3).to_string())&#10;                 except Exception:&#10;                     pass&#10;        except Exception:&#10;            pass&#10;    else:&#10;         # DataFrame vacío con al menos columnas esperadas&#10;         merged = pd.DataFrame()&#10;&#10;    # asegurar existencia de columna _source_type para downstream&#10;    if '_source_type' not in merged.columns:&#10;        merged['_source_type'] = None&#10;&#10;    return merged&#10;&#10;&#10;def _safe_write_parquet(df: pd.DataFrame, path: Path, index: bool = False) -&gt; None:&#10;    &quot;&quot;&quot;Escribe DataFrame a parquet de forma robusta.&#10;&#10;    Intenta escribir con pyarrow; si falla por tipos no esperados, normaliza&#10;    columnas de tipo objeto serializando dicts/listas a JSON y reintenta. Si sigue fallando, fuerza toda la tabla a&#10;    strings como último recurso.&#10;&#10;    Escribe a un fichero temporal y luego reemplaza el destino para evitar corrupciones&#10;    y problemas de concurrencia/locks en Windows. También intenta eliminar el destino&#10;    previo antes de reemplazarlo si surge un permiso.&#10;    &quot;&quot;&quot;&#10;    import os&#10;    import uuid&#10;    import gc&#10;&#10;    # Asegurar directorio destino&#10;    try:&#10;        path.parent.mkdir(parents=True, exist_ok=True)&#10;    except Exception:&#10;        pass&#10;&#10;    # Crear temp_path con uuid para evitar colisiones en re-ejecuciones simultáneas&#10;    temp_path = path.with_name(path.name + f&quot;.{uuid.uuid4().hex}.tmp&quot;)&#10;    # eliminar temp previo si existe (poco probable por uuid)&#10;    try:&#10;        if temp_path.exists():&#10;            temp_path.unlink()&#10;    except Exception:&#10;        pass&#10;&#10;    # Intentar eliminar destino previo antes de escribir para evitar locks en Windows&#10;    def _try_remove_target(p: Path):&#10;        try:&#10;            if p.exists():&#10;                p.unlink()&#10;                return True&#10;        except Exception:&#10;            # intentar renombrar como respaldo (si unlink falla por lock)&#10;            try:&#10;                backup = p.with_name(p.name + &quot;.backup&quot;)&#10;                if backup.exists():&#10;                    backup.unlink()&#10;                p.replace(backup)&#10;                return True&#10;            except Exception:&#10;                return False&#10;        return False&#10;&#10;    def _try_write(df_to_write: pd.DataFrame, target: Path) -&gt; bool:&#10;        try:&#10;            # Forzar engine pyarrow y cerrar recursos inmediatamente&#10;            df_to_write.to_parquet(target, index=index, engine='pyarrow')&#10;            # forzar GC para que pyarrow libere recursos en Windows&#10;            gc.collect()&#10;            return True&#10;        except Exception as e:&#10;            logger.debug('to_parquet falló para %s: %s', target.name, str(e))&#10;            return False&#10;&#10;    # Primer intento: eliminar objetivo si posible (mejora para re-ejecuciones)&#10;    try:&#10;        _try_remove_target(path)&#10;    except Exception:&#10;        pass&#10;&#10;    # Primer intento: escribir directamente al archivo temporal&#10;    try:&#10;        if _try_write(df, temp_path):&#10;            try:&#10;                # Reemplazar atómicamente el destino final (os.replace es atómico en Windows)&#10;                try:&#10;                    os.replace(str(temp_path), str(path))&#10;                except Exception:&#10;                    # fallback a Path.replace&#10;                    temp_path.replace(path)&#10;                logger.info('Escrito %s (replace desde temp)', path.name)&#10;                return&#10;            except Exception as e:&#10;                logger.warning('Fallo al reemplazar %s desde temp: %s. Intentando escritura directa.', path.name, str(e))&#10;                # intentar escribir directamente al destino como último recurso&#10;                try:&#10;                    if _try_write(df, path):&#10;                        logger.info('Escrito %s (directo tras fallo replace)', path.name)&#10;                        try:&#10;                            if temp_path.exists():&#10;                                temp_path.unlink()&#10;                        except Exception:&#10;                            pass&#10;                        return&#10;                except Exception:&#10;                    pass&#10;    except Exception:&#10;        pass&#10;&#10;    logger.warning('Error escribiendo %s. Intentando normalizar columnas objeto.', path.name)&#10;    df2 = df.copy()&#10;    # detectar columnas obj y serializar elementos complejos&#10;    for col in df2.columns:&#10;        if df2[col].dtype == object or df2[col].dtype.name == 'bytes':&#10;            def _conv(v):&#10;                try:&#10;                    if v is None or (isinstance(v, float) and pd.isna(v)):&#10;                        return None&#10;                    # bytes -&gt; str&#10;                    if isinstance(v, (bytes, bytearray)):&#10;                        try:&#10;                            return v.decode('utf-8')&#10;                        except Exception:&#10;                            return str(v)&#10;                    # listas/dicts -&gt; JSON string (preserva estructura)&#10;                    if isinstance(v, (dict, list)):&#10;                        try:&#10;                            return json.dumps(v, ensure_ascii=False)&#10;                        except Exception:&#10;                            return str(v)&#10;                    # numpy types -&gt; native python then str&#10;                    if hasattr(v, 'tolist'):&#10;                        try:&#10;                            return str(v.tolist())&#10;                        except Exception:&#10;                            pass&#10;                    # otros tipos (int, float, etc.) -&gt; str&#10;                    return str(v)&#10;                except Exception:&#10;                    return str(v)&#10;            try:&#10;                df2[col] = df2[col].apply(_conv)&#10;            except Exception:&#10;                # último recurso: convertir toda la columna a string&#10;                try:&#10;                    df2[col] = df2[col].astype(str)&#10;                except Exception:&#10;                    df2[col] = df2[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else str(v))&#10;    # Intentar escribir df2 a temp y reemplazar&#10;    try:&#10;        if _try_write(df2, temp_path):&#10;            try:&#10;                try:&#10;                    os.replace(str(temp_path), str(path))&#10;                except Exception:&#10;                    temp_path.replace(path)&#10;                logger.info('Escrito (normalizado) %s', path.name)&#10;                return&#10;            except Exception as e2:&#10;                logger.warning('Fallo al reemplazar %s tras normalizar: %s', path.name, str(e2))&#10;                try:&#10;                    if _try_write(df2, path):&#10;                        logger.info('Escrito %s (directo, normalizado)', path.name)&#10;                        try:&#10;                            if temp_path.exists():&#10;                                temp_path.unlink()&#10;                        except Exception:&#10;                            pass&#10;                        return&#10;                except Exception:&#10;                    pass&#10;    except Exception:&#10;        pass&#10;&#10;    logger.warning('Fallo escribiendo %s tras normalizar columnas. Forzando todo a strings.', path.name)&#10;    # Último intento: forzar todas las columnas a strings (None -&gt; None)&#10;    df3 = df.copy()&#10;    for col in df3.columns:&#10;        try:&#10;            df3[col] = df3[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else str(v))&#10;        except Exception:&#10;            try:&#10;                df3[col] = df3[col].astype(str)&#10;            except Exception:&#10;                df3[col] = df3[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else str(v))&#10;    # Escribir df3 a temp y reemplazar&#10;    try:&#10;        if _try_write(df3, temp_path):&#10;            try:&#10;                try:&#10;                    os.replace(str(temp_path), str(path))&#10;                except Exception:&#10;                    temp_path.replace(path)&#10;                logger.info('Escrito (fallback strings) %s', path.name)&#10;                return&#10;            except Exception as e3:&#10;                # último recurso: escribir directo&#10;                try:&#10;                    df3.to_parquet(path, index=index, engine='pyarrow')&#10;                    logger.info('Escrito (fallback directo) %s', path.name)&#10;                    try:&#10;                        if temp_path.exists():&#10;                            temp_path.unlink()&#10;                    except Exception:&#10;                        pass&#10;                    return&#10;                except Exception as e4:&#10;                    logger.error('Fallo definitivo al escribir %s: %s', path.name, str(e4))&#10;                    raise&#10;    except Exception as e:&#10;        logger.error('Fallo definitivo al escribir %s: %s', path.name, str(e))&#10;        raise&#10;&#10;def safe_compute_quality_metrics(rows):&#10;    &quot;&quot;&quot;Wrapper seguro para compute_quality_metrics: si falla la función original,&#10;    devuelve métricas mínimas que permiten continuar y rastrear el error.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        return compute_quality_metrics(rows)&#10;    except Exception as e:&#10;        logger.warning(&quot;safe_compute_quality_metrics: compute_quality_metrics falló: %s&quot;, str(e))&#10;        # Fallback mínimo&#10;        total = len(rows) if isinstance(rows, list) else 0&#10;        nulos = {}&#10;        try:&#10;            cols = [&quot;book_id&quot;, &quot;titulo&quot;, &quot;autor_principal&quot;, &quot;isbn13&quot;, &quot;fecha_publicacion&quot;, &quot;idioma&quot;, &quot;precio&quot;, &quot;moneda&quot;]&#10;            nulos = nulls_by_column(rows, cols) if rows else {c: 0 for c in cols}&#10;        except Exception:&#10;            nulos = {}&#10;        return {&#10;            &quot;filas_totales&quot;: total,&#10;            &quot;nulos_por_campo&quot;: nulos,&#10;            &quot;porcentaje_fechas_validas&quot;: 0.0,&#10;            &quot;porcentaje_idiomas_validos&quot;: 0.0,&#10;            &quot;porcentaje_monedas_validas&quot;: 0.0,&#10;            &quot;porcentaje_isbn13_validos&quot;: 0.0,&#10;            &quot;completitud_promedio&quot;: 0.0,&#10;            &quot;duplicados_isbn13&quot;: 0,&#10;            &quot;duplicados_book_id&quot;: 0,&#10;            &quot;filas_por_fuente&quot;: {},&#10;        }&#10;&#10;def _canonicalize_merged_columns(merged: pd.DataFrame) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;Asegura nombres de columnas consistentes y tipos básicos para downstream.&#10;    - Normaliza sufijos si existen mezclas (_gr/_gb)&#10;    - Crea columnas _source_title/_source_author auxiliares si procede&#10;    KEYWORDS: CANONICALIZE_COLUMNS&#10;    &quot;&quot;&quot;&#10;    if not isinstance(merged, pd.DataFrame):&#10;        return merged&#10;    df = merged.copy()&#10;    # eliminar duplicados accidentales en nombres (ej. col y col_gr ambos presentes)&#10;    for col in list(df.columns):&#10;        if col.endswith('_gr') or col.endswith('_gb'):&#10;            base = col[:-3]&#10;            # si existe la columna base y la columna sufijada tiene datos más completos, preferir sufijo&#10;            try:&#10;                if base in df.columns:&#10;                    # preferir valores no nulos de sufijo sobre base&#10;                    df[base] = df[base].where(df[base].notna(), df[col])&#10;            except Exception:&#10;                pass&#10;    # asegurar columnas clave presentes&#10;    for expected in ['titulo','autor_principal','isbn13','isbn10','precio','moneda','fecha_publicacion','autores','categoria']:&#10;        if expected not in df.columns:&#10;            df[expected] = None&#10;    return df&#10;&#10;&#10;def _apply_semantic_normalization(df: pd.DataFrame) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;Normaliza fechas, idiomas, moneda y precio, y añade flags de validez.&#10;    Operación vectorizada en la medida de lo posible para rendimiento.&#10;    KEYWORDS: NORMALIZE_SEMANTIC&#10;    &quot;&quot;&quot;&#10;    if not isinstance(df, pd.DataFrame):&#10;        return df&#10;    out = df.copy()&#10;    # Normalizar fecha_publicacion -&gt; ISO y marcar parcial&#10;    if 'fecha_publicacion' in out.columns:&#10;        iso_dates = []&#10;        parcial_flags = []&#10;        valid_flags = []&#10;        for v in out['fecha_publicacion'].fillna('').astype(str).tolist():&#10;            if not v or v.strip() == '' or v == 'nan':&#10;                iso_dates.append(None)&#10;                parcial_flags.append(False)&#10;                valid_flags.append(False)&#10;            else:&#10;                try:&#10;                    iso, parcial = parse_date_to_iso(v)&#10;                    iso_dates.append(iso)&#10;                    parcial_flags.append(bool(parcial))&#10;                    valid_flags.append(bool(iso is not None))&#10;                except Exception:&#10;                    iso_dates.append(None)&#10;                    parcial_flags.append(False)&#10;                    valid_flags.append(False)&#10;        out['fecha_publicacion'] = iso_dates&#10;        out['fecha_publicacion_parcial'] = parcial_flags&#10;        out['fecha_publicacion_valida'] = valid_flags&#10;    else:&#10;        out['fecha_publicacion_parcial'] = False&#10;        out['fecha_publicacion_valida'] = False&#10;&#10;    # Normalizar idioma&#10;    if 'idioma' in out.columns:&#10;        norm_langs = []&#10;        lang_valid = []&#10;        for v in out['idioma'].fillna('').astype(str).tolist():&#10;            if not v or v.strip() == '' or v == 'nan':&#10;                norm_langs.append(None)&#10;                lang_valid.append(False)&#10;            else:&#10;                try:&#10;                    nl = normalize_language(v)&#10;                    ok = validate_language(nl)&#10;                    norm_langs.append(nl)&#10;                    lang_valid.append(bool(ok))&#10;                except Exception:&#10;                    norm_langs.append(None)&#10;                    lang_valid.append(False)&#10;        out['idioma'] = norm_langs&#10;        out['idioma_valido'] = lang_valid&#10;    else:&#10;        out['idioma_valido'] = False&#10;&#10;    # Normalizar moneda&#10;    if 'moneda' in out.columns:&#10;        norm_cur = []&#10;        cur_valid = []&#10;        for v in out['moneda'].fillna('').astype(str).tolist():&#10;            if not v or v.strip() == '' or v == 'nan':&#10;                norm_cur.append(None)&#10;                cur_valid.append(False)&#10;            else:&#10;                try:&#10;                    nc = normalize_currency(v)&#10;                    ok = validate_currency(nc) if nc else False&#10;                    norm_cur.append(nc)&#10;                    cur_valid.append(bool(ok))&#10;                except Exception:&#10;                    norm_cur.append(None)&#10;                    cur_valid.append(False)&#10;        out['moneda'] = norm_cur&#10;        out['moneda_valida'] = cur_valid&#10;    else:&#10;        out['moneda_valida'] = False&#10;&#10;    # Normalizar precio -&gt; float coercion&#10;    if 'precio' in out.columns:&#10;        precs = []&#10;        for v in out['precio'].tolist():&#10;            if v is None or (isinstance(v, float) and pd.isna(v)) or (isinstance(v, str) and v.strip() == ''):&#10;                precs.append(None)&#10;            else:&#10;                try:&#10;                    # limpiar comas y espacios; forzar punto decimal&#10;                    s = str(v).strip()&#10;                    s = s.replace(',', '.')&#10;                    # eliminar símbolos excepto digits, dot and minus&#10;                    s2 = ''.join([c for c in s if (c.isdigit() or c in '.-')])&#10;                    precs.append(float(s2))&#10;                except Exception:&#10;                    precs.append(None)&#10;        out['precio'] = precs&#10;    return out&#10;&#10;&#10;def integrate() -&gt; None:&#10;    &quot;&quot;&quot;Integración completa: carga, normalización, fusión y escritura de artefactos estándar.&#10;    Ejecutar en el orden correcto es crítico: primero se cargan y normalizan las fuentes,&#10;    luego se fusionan, se generan métricas de calidad y finalmente se escriben los resultados.&#10;    &quot;&quot;&quot;&#10;    global gr_n, gb_n, price_map, detail, assertions&#10;&#10;    # Inicializaciones defensivas para evitar referencias no resueltas en flujos excepcionales&#10;    dim = pd.DataFrame()&#10;    dim_for_metrics = pd.DataFrame()&#10;    detail = pd.DataFrame()&#10;    assertions = {}&#10;&#10;    # Asegurar directorios de logs de trabajo (work/logs/..)&#10;    try:&#10;        ensure_work_dirs()&#10;    except Exception:&#10;        # no bloquear si no existe work/utils_logging&#10;        pass&#10;&#10;    # Eliminar artefactos previos para evitar problemas de locks en Windows y re-ejecuciones&#10;    try:&#10;        prev_dim = STANDARD / 'dim_book.parquet'&#10;        prev_detail = STANDARD / 'book_source_detail.parquet'&#10;        if prev_dim.exists():&#10;            try:&#10;                prev_dim.unlink()&#10;                logger.info('Eliminado artefacto previo: %s', prev_dim.name)&#10;            except Exception:&#10;                logger.debug('No se pudo eliminar previo dim_book.parquet', exc_info=True)&#10;        if prev_detail.exists():&#10;            try:&#10;                prev_detail.unlink()&#10;                logger.info('Eliminado artefacto previo: %s', prev_detail.name)&#10;            except Exception:&#10;                logger.debug('No se pudo eliminar previo book_source_detail.parquet', exc_info=True)&#10;    except Exception:&#10;        pass&#10;&#10;    # Carga inicial de fuentes&#10;    gr_n, gb_n = _load_sources()&#10;&#10;    # Normalización básica&#10;    gr_n, gb_n = _normalize_frames(gr_n, gb_n)&#10;&#10;    # Merge y asignación de ISBN desde candidatos&#10;    merged = _merge_sources(gr_n, gb_n)&#10;&#10;    # Normalizar columnas canónicas para prevenir errores posteriores&#10;    try:&#10;        merged = _canonicalize_merged_columns(merged)&#10;    except Exception:&#10;        logger.debug('Fallo al canonicalizar columnas de merged; se mantiene objeto original', exc_info=True)&#10;&#10;    # APLICAR NORMALIZACIÓN SEMÁNTICA: fechas ISO, idioma BCP-47, moneda ISO-4217, precio numérico&#10;    try:&#10;        merged = _apply_semantic_normalization(merged)&#10;    except Exception:&#10;        logger.debug('Normalización semántica falló; se continúa con merged original', exc_info=True)&#10;&#10;    # Unificar columnas base (coalesce) para que existan nombres sin sufijos (titulo, autor_principal, isbn13, etc.)&#10;    try:&#10;        base_cols = [&#10;            'titulo','autor_principal','autores','editorial','anio_publicacion','fecha_publicacion','idioma','isbn10','isbn13',&#10;            'precio','moneda','categoria','ts_ultima_actualizacion','isbn13_valido','idioma_valido','moneda_valida','fecha_publicacion_valida','fecha_publicacion_parcial','gb_match_score'&#10;        ]&#10;        for col in base_cols:&#10;            series = None&#10;            # considerar tanto formas con sufijos como las canónicas ya presentes&#10;            candidates = [col, f&quot;{col}_gr&quot;, f&quot;{col}_gb&quot;, f&quot;{col}_gr&quot;.replace('_gr','_gr'), f&quot;{col}_gb&quot;.replace('_gb','_gb')]&#10;            for key in candidates:&#10;                if key in merged.columns:&#10;                    s = merged[key]&#10;                    if series is None:&#10;                        series = s.copy()&#10;                    else:&#10;                        try:&#10;                            series = series.fillna(s)&#10;                        except Exception:&#10;                            series = series.where(series.notna(), s)&#10;            if series is None:&#10;                merged[col] = pd.Series([None] * len(merged), index=merged.index)&#10;            else:&#10;                merged[col] = series&#10;    except Exception:&#10;        logger.debug('Coalesce de columnas base falló, se continúa con merged tal cual', exc_info=True)&#10;&#10;    # Separar merged en dos vistas: una completa para detalle (detail) y otra filtrada para construir dim (excluir gb_only)&#10;    try:&#10;        merged_for_dim = merged[merged['_source_type'] != 'gb_only'].copy() if isinstance(merged, pd.DataFrame) else pd.DataFrame()&#10;    except Exception:&#10;        merged_for_dim = merged.copy() if isinstance(merged, pd.DataFrame) else pd.DataFrame()&#10;&#10;    # ===== Construir dim canónico a partir de merged_for_dim =====&#10;    try:&#10;        # Intentar construir la tabla dimensional con la función dedicada&#10;        dim = _build_dim_from_merged(merged_for_dim)&#10;&#10;        # Asignar book_id: preferir isbn13 válido, sino canonical_key&#10;        try:&#10;            if 'isbn13' in dim.columns and 'isbn13_valido' in dim.columns:&#10;                dim['book_id'] = dim.apply(lambda r: r['isbn13'] if (pd.notna(r.get('isbn13')) and bool(r.get('isbn13_valido'))) else r.get('canonical_key'), axis=1)&#10;            elif 'canonical_key' in dim.columns:&#10;                dim['book_id'] = dim['canonical_key']&#10;            else:&#10;                dim['book_id'] = None&#10;        except Exception:&#10;            # fallback genérico&#10;            if 'canonical_key' in dim.columns:&#10;                dim['book_id'] = dim['canonical_key']&#10;            else:&#10;                dim['book_id'] = None&#10;&#10;        # Asegurar ts_ultima_actualizacion&#10;        try:&#10;            now_ts = datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;            if 'ts_ultima_actualizacion' not in dim.columns:&#10;                dim['ts_ultima_actualizacion'] = now_ts&#10;            else:&#10;                dim['ts_ultima_actualizacion'] = dim['ts_ultima_actualizacion'].fillna(now_ts)&#10;        except Exception:&#10;            dim['ts_ultima_actualizacion'] = datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;&#10;        # Deduplicar por book_id garantizando una fila por libro canónico&#10;        try:&#10;            if 'book_id' in dim.columns:&#10;                # preferir filas con más campos no nulos y con fuente_ganadora&#10;                # calcular score simple: número de campos no nulos&#10;                score_cols = ['titulo','autor_principal','editorial','fecha_publicacion','autores','precio','moneda']&#10;                dim['_completitud_tmp'] = dim[score_cols].notna().sum(axis=1)&#10;                # si existe gb_match_score, añadirlo al score&#10;                if 'gb_match_score' in dim.columns:&#10;                    try:&#10;                        dim['_gb_ms_tmp'] = pd.to_numeric(dim['gb_match_score'], errors='coerce').fillna(0.0)&#10;                        dim['_score_total'] = dim['_completitud_tmp'] + dim['_gb_ms_tmp']&#10;                    except Exception:&#10;                        dim['_score_total'] = dim['_completitud_tmp']&#10;                else:&#10;                    dim['_score_total'] = dim['_completitud_tmp']&#10;                dim = dim.sort_values(by=['_score_total'], ascending=False).drop_duplicates(subset=['book_id'], keep='first').reset_index(drop=True)&#10;                # eliminar columnas temporales&#10;                for _c in ['_completitud_tmp','_gb_ms_tmp','_score_total']:&#10;                    if _c in dim.columns:&#10;                        try:&#10;                            del dim[_c]&#10;                        except Exception:&#10;                            pass&#10;        except Exception:&#10;            # si falla deduplicación, no interrumpir; dim se mantiene&#10;            pass&#10;    except Exception as e:&#10;        # No bloquear pipeline si la construcción del dim falla; dejar que el flujo posterior use el fallback&#10;        logger.warning('No se pudo construir dim desde merged_for_dim: %s', str(e))&#10;        dim = pd.DataFrame()&#10;&#10;    # Registro de diagnóstico: tamaño de merged y unicidad de book_id antes de dedup&#10;    try:&#10;        logger.info(&#10;            'Merged rows (total) antes dedup: %d; merged_for_dim rows: %d',&#10;            0 if merged is None else (len(merged) if hasattr(merged, '__len__') else 0),&#10;            0 if merged_for_dim is None else (len(merged_for_dim) if hasattr(merged_for_dim, '__len__') else 0),&#10;        )&#10;        if isinstance(merged, pd.DataFrame) and not merged.empty:&#10;            # contar book_id posibles (puede no existir aún)&#10;            if 'book_id' in merged.columns:&#10;                counts = merged['book_id'].value_counts(dropna=False)&#10;                dup_groups = counts[counts &gt; 1]&#10;                logger.info('book_id únicos en merged: %d, grupos duplicados: %d', counts.shape[0], dup_groups.shape[0])&#10;                if not dup_groups.empty:&#10;                    logger.debug('book_id duplicados (muestra): %s', dup_groups.head(10).to_dict())&#10;            else:&#10;                logger.info('book_id no presente aún en merged')&#10;    except Exception:&#10;        logger.debug('Diagnóstico merged falló', exc_info=True)&#10;&#10;    # --- Aserciones soft y deduplicación ---&#10;    # Construir book_source_detail a partir de 'merged' y aplicar reglas de exclusión suaves&#10;    try:&#10;        # convertir merged (DataFrame) en uno con columnas esperadas&#10;        # detail = merged.copy()&#10;        # fila original (row_number) para trazabilidad&#10;        # detail['row_number'] = detail.index + 1&#10;        # detail['book_id_candidato'] = detail.get('book_id')&#10;        # detail['ts_ingesta'] = detail.get('ts_ultima_actualizacion') if 'ts_ultima_actualizacion' in detail.columns else datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;&#10;        # Construir `book_source_detail` preservando las filas originales de landing&#10;        # y rellenando campos de Goodreads desde GoogleBooks cuando sea posible.&#10;        # _build_book_source_detail devuelve un DataFrame con las columnas exactas&#10;        # requeridas y `ts_ingest` con la hora de ejecución.&#10;        try:&#10;            detail = _build_book_source_detail(gr_n, gb_n)&#10;        except Exception as _e:&#10;            logger.warning('Fallo construyendo book_source_detail desde landing: %s. Usando merged como fallback.', str(_e))&#10;            detail = merged.copy()&#10;            # mantener compatibilidad: asignar row_number y ts_ingesta si faltan&#10;            if 'row_number' not in detail.columns:&#10;                detail['row_number'] = detail.index + 1&#10;            if 'book_id_candidato' not in detail.columns:&#10;                detail['book_id_candidato'] = detail.get('book_id')&#10;            if 'ts_ingest' not in detail.columns:&#10;                detail['ts_ingest'] = detail.get('ts_ultima_actualizacion') if 'ts_ultima_actualizacion' in detail.columns else datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;    except Exception as e:&#10;        logger.warning('Error en fase de deduplicación/aserciones: %s', str(e))&#10;        logger.debug(traceback.format_exc())&#10;        # fallback: usar merged como dim y un detail mínimo&#10;        dim = merged.copy()&#10;        detail = merged.copy()&#10;        detail['row_number'] = detail.index + 1&#10;        detail['book_id_candidato'] = detail.get('book_id')&#10;        detail['ts_ingesta'] = detail.get('ts_ultima_actualizacion') if 'ts_ultima_actualizacion' in detail.columns else datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;        detail['valid'] = True&#10;        detail['exclude_reason'] = None&#10;&#10;    # APLICAR LIMPIEZA DE REFERENCIAS ANTES DE ESCRIBIR PARA EVITAR BLOQUEOS EN WINDOWS&#10;    try:&#10;        import gc&#10;        # eliminar referencias pesadas temporales&#10;        for _n in ('merged_local','merged_by_isbn','merged_by_key','gb_only_prefixed','to_concat','merged'):&#10;            try:&#10;                if _n in locals():&#10;                    del locals()[_n]&#10;            except Exception:&#10;                pass&#10;        gc.collect()&#10;    except Exception:&#10;        pass&#10;&#10;    # Métricas de calidad&#10;    records_sanitized: List[Mapping[str, object]] = []&#10;    try:&#10;        # usar dim deduplicado y final para métricas (asegurar 1 fila por libro canónico)&#10;        try:&#10;            # deduplicar por isbn13 preferente, si no usar book_id&#10;            dim_for_metrics = dim.copy()&#10;            if 'isbn13' in dim_for_metrics.columns and dim_for_metrics['isbn13'].notna().any():&#10;                dim_for_metrics = dim_for_metrics.sort_values(by=['isbn13','gb_match_score'], ascending=[True, False]).drop_duplicates(subset=['isbn13'], keep='first').reset_index(drop=True)&#10;            else:&#10;                dim_for_metrics = dim_for_metrics.drop_duplicates(subset=['book_id'], keep='first').reset_index(drop=True)&#10;            records = dim_for_metrics.to_dict(orient='records')&#10;        except Exception as e:&#10;            logger.warning('dim.to_dict falló para métricas: %s. Intentando fallback row-wise.', str(e))&#10;            records = []&#10;            try:&#10;                for _, row in dim.iterrows():&#10;                    try:&#10;                        # construir dict simple con columnas planas&#10;                        rec = {c: _sanitize_value(row.get(c)) for c in dim.columns}&#10;                        records.append(rec)&#10;                    except Exception:&#10;                        records.append({})&#10;            except Exception:&#10;                records = []&#10;        # Sanitizar registros para evitar tipos complejos que rompan compute_quality_metrics&#10;        try:&#10;            records_sanitized = _sanitize_records_for_metrics(records)&#10;        except Exception as e:&#10;            logger.warning('Sanitización de records falló: %s. Intentando sanitizar manualmente.', str(e))&#10;            records_sanitized = []&#10;            for r in (records or []):&#10;                if isinstance(r, Mapping):&#10;                    row = {}&#10;                    for k, v in r.items():&#10;                        try:&#10;                            row[k] = _sanitize_value(v)&#10;                        except Exception:&#10;                            row[k] = None&#10;                    records_sanitized.append(row)&#10;                else:&#10;                    try:&#10;                        records_sanitized.append({'raw': _sanitize_value(r)})&#10;                    except Exception:&#10;                        records_sanitized.append({})&#10;        # usar wrapper seguro para evitar que una excepción interrumpa la escritura&#10;        assertions = safe_compute_quality_metrics(records_sanitized)&#10;        # Forzar valores clave coherentes: filas_totales = número de filas del dim final&#10;        try:&#10;            assertions['filas_totales'] = len(records_sanitized)&#10;        except Exception:&#10;            pass&#10;        # filas_por_fuente: contar por 'fuente_ganadora' en dim_for_metrics si existe&#10;        try:&#10;            if isinstance(dim_for_metrics, pd.DataFrame) and 'fuente_ganadora' in dim_for_metrics.columns:&#10;                fps = dim_for_metrics['fuente_ganadora'].fillna('unknown').value_counts().to_dict()&#10;                assertions['filas_por_fuente'] = fps&#10;            else:&#10;                # fallback: contar por source_name columns in detail&#10;                assertions.setdefault('filas_por_fuente', {})&#10;        except Exception:&#10;            pass&#10;    except Exception as e:&#10;        logger.warning('Error calculando métricas de calidad: %s', str(e))&#10;        logger.debug(traceback.format_exc())&#10;        assertions = safe_compute_quality_metrics([])&#10;&#10;    # Registrar una entrada mínima en logs/rules para trazabilidad (evita carpeta vacía)&#10;    try:&#10;        log_rule_jsonl({&#10;            &quot;event&quot;: &quot;integrate_run_summary&quot;,&#10;            &quot;ts_utc&quot;: datetime.now(timezone.utc).isoformat(timespec='seconds'),&#10;            &quot;records_input&quot;: assertions.get('filas_totales') if isinstance(assertions, dict) else None,&#10;            &quot;notes&quot;: &quot;integrate_pipeline executed&quot;,&#10;        })&#10;    except Exception:&#10;        pass&#10;&#10;    # Escritura de artefactos estándar&#10;    try:&#10;        # normalizar tipos en dim para evitar problemas con pyarrow&#10;        for col in dim.columns:&#10;            if dim[col].dtype == object:&#10;                # serializar dict/list/bytes&#10;                dim[col] = dim[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else (v.decode('utf-8') if isinstance(v, (bytes, bytearray)) else (json.dumps(v, ensure_ascii=False) if isinstance(v, (dict, list)) else v)))&#10;        # liberar referencias pesadas antes de escribir para evitar locks en Windows&#10;        try:&#10;            import gc&#10;            # eliminar referencias que ya no se necesitan&#10;            for _name in ('gr_n', 'gb_n', 'merged', 'merged_local', 'merged_by_isbn', 'merged_by_key', 'gb_only'):&#10;                try:&#10;                    if _name in globals():&#10;                        del globals()[_name]&#10;                except Exception:&#10;                    pass&#10;            gc.collect()&#10;        except Exception:&#10;            pass&#10;        _safe_write_parquet(dim, STANDARD / 'dim_book.parquet', index=False)&#10;    except Exception as e:&#10;        logger.error('Fallo al escribir dim_book.parquet: %s', str(e))&#10;        raise&#10;&#10;    try:&#10;        if not isinstance(detail, pd.DataFrame):&#10;            detail = pd.DataFrame(detail)&#10;        # normalizar detail: asegurar tipos simples&#10;        for col in detail.columns:&#10;            if detail[col].dtype == object:&#10;                # aplicar _sanitize_value por elemento para evitar listas/dicts/bytes&#10;                try:&#10;                    detail[col] = detail[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else _sanitize_value(v))&#10;                except Exception:&#10;                    # último recurso: convertir a string seguro&#10;                    detail[col] = detail[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else str(v))&#10;        # asegurar row_number entero&#10;        if 'row_number' in detail.columns:&#10;            try:&#10;                detail['row_number'] = pd.to_numeric(detail['row_number'], errors='coerce').astype('Int64')&#10;            except Exception:&#10;                pass&#10;        _safe_write_parquet(detail, STANDARD / 'book_source_detail.parquet', index=False)&#10;    except Exception as e:&#10;        logger.error('Fallo al escribir book_source_detail.parquet: %s', str(e))&#10;        # intentar escribir una versión reducida para diagnóstico&#10;        try:&#10;            diag = detail.copy()&#10;            # limitar columnas a las más relevantes y sanitizar&#10;            keep = [c for c in ['row_number','book_id_candidato','titulo','autor_principal','isbn13','fecha_publicacion','precio','moneda','valid','exclude_reason'] if c in diag.columns]&#10;            diag = diag[keep]&#10;            for col in diag.columns:&#10;                diag[col] = diag[col].apply(lambda v: None if (v is None or (isinstance(v, float) and pd.isna(v))) else _sanitize_value(v))&#10;            _safe_write_parquet(diag, STANDARD / 'book_source_detail.parquet', index=False)&#10;            logger.info('Escrito book_source_detail.parquet (diagnóstico reducido)')&#10;        except Exception as e2:&#10;            logger.error('Fallo final al escribir book_source_detail.parquet (diagnóstico): %s', str(e2))&#10;            raise&#10;&#10;    try:&#10;        with open(DOCS / 'quality_metrics.json', 'w', encoding='utf-8') as f:&#10;            json.dump(assertions, f, ensure_ascii=False, indent=2)&#10;    except Exception as e:&#10;        logger.warning('No se pudo escribir quality_metrics.json: %s', str(e))&#10;&#10;    logger.info('Escrito dim_book.parquet y book_source_detail.parquet; métricas en quality_metrics.json')&#10;&#10;&#10;def main() -&gt; None:&#10;    integrate()&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;&#10;def _coalesce_columns(df: pd.DataFrame, col: str, suffixes: List[str] = None) -&gt; pd.Series:&#10;    # Intenta combinar valores de columnas con sufijos diferentes en una sola columna.&#10;    # Prioriza valores no nulos de columnas con sufijo sobre la columna base.&#10;    if suffixes is None:&#10;        suffixes = ['_gr', '_gb']&#10;    candidates = [col] + [f&quot;{col}{s}&quot; for s in suffixes]&#10;    series = None&#10;    for c in candidates:&#10;        if c in df.columns:&#10;            s = df[c]&#10;            if series is None:&#10;                series = s.astype(object)&#10;            else:&#10;                try:&#10;                    series = series.combine_first(s)&#10;                except Exception:&#10;                    series = series.where(series.notna(), s)&#10;    if series is None:&#10;        series = pd.Series([None] * len(df), index=df.index, dtype=object)&#10;    return series&#10;&#10;&#10;def _merge_list_fields(val_a, val_b):&#10;    # Mezcla dos campos de lista (separados por ';') o valores escalares, eliminando duplicados.&#10;    try:&#10;        a = []&#10;        b = []&#10;        if val_a is not None and (not (isinstance(val_a, float) and pd.isna(val_a))):&#10;            if isinstance(val_a, str):&#10;                a = [x.strip() for x in val_a.split(';') if x.strip()]&#10;            elif isinstance(val_a, (list, tuple)):&#10;                a = [str(x).strip() for x in val_a if x is not None]&#10;        if val_b is not None and (not (isinstance(val_b, float) and pd.isna(val_b))):&#10;            if isinstance(val_b, str):&#10;                b = [x.strip() for x in val_b.split(';') if x.strip()]&#10;            elif isinstance(val_b, (list, tuple)):&#10;                b = [str(x).strip() for x in val_b if x is not None]&#10;        merged = []&#10;        for x in a + b:&#10;            if x and x not in merged:&#10;                merged.append(x)&#10;        if merged:&#10;            return ';'.join(merged)&#10;    except Exception:&#10;        pass&#10;    return None&#10;&#10;&#10;def _compute_provenance(row: pd.Series, base_fields: List[str]) -&gt; str:&#10;    # Computa la procedencia de los datos en una fila dada, basado en los campos base.&#10;    prov = {}&#10;    try:&#10;        for f in base_fields:&#10;            src = None&#10;            gr_key = f&quot;{f}_gr&quot;&#10;            gb_key = f&quot;{f}_gb&quot;&#10;            if gr_key in row.index and pd.notna(row.get(gr_key)) and str(row.get(gr_key)).strip() != '':&#10;                src = 'goodreads'&#10;            elif gb_key in row.index and pd.notna(row.get(gb_key)) and str(row.get(gb_key)).strip() != '':&#10;                src = 'google_books'&#10;            else:&#10;                if f in row.index and pd.notna(row.get(f)) and str(row.get(f)).strip() != '':&#10;                    st = row.get('_source_type')&#10;                    if isinstance(st, str) and st.startswith('merged_by_'):&#10;                        src = 'merged'&#10;                    elif st == 'gb_only':&#10;                        src = 'google_books'&#10;                    elif st == 'gr_only':&#10;                        src = 'goodreads'&#10;                    else:&#10;                        src = 'unknown'&#10;            prov[f] = src&#10;    except Exception:&#10;        for f in base_fields:&#10;            prov[f] = None&#10;    try:&#10;        return json.dumps(prov, ensure_ascii=False)&#10;    except Exception:&#10;        return json.dumps({f: prov.get(f) for f in base_fields}, ensure_ascii=False)&#10;&#10;&#10;def _build_book_source_detail(gr: pd.DataFrame, gb: pd.DataFrame) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;Construye el detalle de la fuente de los libros a partir de las fuentes de Goodreads y Google Books.&#10;&#10;    Combina registros de ambas fuentes, asignando campos comunes y marcando la procedencia.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        ts = datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;        gr_copy = gr.copy()&#10;        # Asegurar nombres de columna consistentes (titulo, autor_principal, isbn13, isbn10, fecha_publicacion)&#10;        if 'title' in gr_copy.columns and 'titulo' not in gr_copy.columns:&#10;            gr_copy.rename(columns={'title': 'titulo'}, inplace=True)&#10;        if 'author' in gr_copy.columns and 'autor_principal' not in gr_copy.columns:&#10;            gr_copy.rename(columns={'author': 'autor_principal'}, inplace=True)&#10;&#10;        if 'row_number' not in gr_copy.columns:&#10;            try:&#10;                gr_copy['row_number'] = (gr_copy.index.astype(int) + 1)&#10;            except Exception:&#10;                gr_copy['row_number'] = list(range(1, len(gr_copy) + 1))&#10;        gr_copy['source_name'] = 'goodreads'&#10;        gr_copy['source_file'] = str((LANDING / 'goodreads_books.json').name)&#10;        gr_copy['ts_ingest'] = ts&#10;        # book_id_candidato: preferir isbn13 si existe&#10;        gr_copy['book_id_candidato'] = gr_copy.get('isbn13')&#10;        if 'isbn13' in gr_copy.columns:&#10;            gr_copy['isbn13_valido'] = gr_copy['isbn13'].apply(lambda v: bool(is_valid_isbn13(str(v))) if pd.notna(v) and str(v).strip() != '' else False)&#10;        else:&#10;            gr_copy['isbn13_valido'] = False&#10;        gr_copy['fecha_publicacion_valida'] = gr_copy.get('fecha_publicacion').apply(lambda v: bool(parse_date_to_iso(v)[0]) if pd.notna(v) and str(v).strip() != '' else False) if 'fecha_publicacion' in gr_copy.columns else False&#10;        # Corregir nombre de flag para idioma (consistencia)&#10;        gr_copy['idioma_valido'] = False&#10;        gr_copy['moneda_valida'] = False&#10;&#10;        gb_copy = gb.copy()&#10;        # Asegurar nombres de columna en gb&#10;        if 'title' in gb_copy.columns and 'titulo' not in gb_copy.columns:&#10;            gb_copy.rename(columns={'title': 'titulo'}, inplace=True)&#10;        if 'authors' in gb_copy.columns and 'autores' not in gb_copy.columns:&#10;            gb_copy.rename(columns={'authors': 'autores'}, inplace=True)&#10;        if '_csv_row' in gb_copy.columns:&#10;            gb_copy['row_number'] = gb_copy['_csv_row']&#10;        else:&#10;            try:&#10;                gb_copy['row_number'] = (gb_copy.index.astype(int) + 1)&#10;            except Exception:&#10;                gb_copy['row_number'] = list(range(1, len(gb_copy) + 1))&#10;        gb_copy['source_name'] = 'google_books'&#10;        gb_copy['source_file'] = str((LANDING / 'googlebooks_books.csv').name)&#10;        gb_copy['ts_ingest'] = ts&#10;        gb_copy['book_id_candidato'] = gb_copy.get('isbn13')&#10;        if 'isbn13' in gb_copy.columns:&#10;            gb_copy['isbn13_valido'] = gb_copy['isbn13'].apply(lambda v: bool(is_valid_isbn13(str(v))) if pd.notna(v) and str(v).strip() != '' else False)&#10;        else:&#10;            gb_copy['isbn13_valido'] = False&#10;        gb_copy['fecha_publicacion_valida'] = gb_copy.get('fecha_publicacion').apply(lambda v: bool(parse_date_to_iso(v)[0]) if pd.notna(v) and str(v).strip() != '' else False) if 'fecha_publicacion' in gb_copy.columns else False&#10;        if 'idioma' in gb_copy.columns:&#10;            gb_copy['idioma_valido'] = gb_copy['idioma'].apply(lambda v: bool(validate_language(normalize_language(v))) if pd.notna(v) and str(v).strip() != '' else False)&#10;        else:&#10;            gb_copy['idioma_valido'] = False&#10;        if 'moneda' in gb_copy.columns:&#10;            gb_copy['moneda_valida'] = gb_copy['moneda'].apply(lambda v: bool(validate_currency(normalize_currency(v))) if pd.notna(v) and str(v).strip() != '' else False)&#10;        else:&#10;            gb_copy['moneda_valida'] = False&#10;&#10;        if 'autores' in gb_copy.columns:&#10;            gb_copy['autores'] = gb_copy['autores'].apply(lambda x: ';'.join(uniq_preserve(listify(x))) if pd.notna(x) else None)&#10;        if 'categoria' in gb_copy.columns:&#10;            gb_copy['categoria'] = gb_copy['categoria'].apply(lambda x: ';'.join(uniq_preserve(listify(x))) if pd.notna(x) else None)&#10;&#10;        detail = pd.concat([gr_copy.reset_index(drop=True), gb_copy.reset_index(drop=True)], ignore_index=True, sort=False)&#10;        try:&#10;            detail['row_number'] = pd.to_numeric(detail['row_number'], errors='coerce').astype('Int64')&#10;        except Exception:&#10;            pass&#10;        if 'ts_ingest' not in detail.columns:&#10;            detail['ts_ingest'] = ts&#10;        return detail&#10;    except Exception as e:&#10;        raise RuntimeError(f&quot;_build_book_source_detail falló: {str(e)}&quot;)&#10;&#10;&#10;def _build_dim_from_merged(merged: pd.DataFrame) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;Construye la tabla dimensional a partir del DataFrame combinado (merged).&#10;&#10;    Objetivo ajustado: producir un dim cuyo esquema principal sea el de Goodreads&#10;    (campos extraídos del JSON original) y rellenar los campos incompletos con&#10;    los valores provenientes de Google Books cuando estén disponibles.&#10;&#10;    Salida: DataFrame con, como mínimo, las columnas:&#10;      - titulo, autor_principal, rating, ratings_count, book_url, isbn10, isbn13&#10;    y columnas adicionales tomadas de Google Books si aportan información:&#10;      - editorial, fecha_publicacion (ISO), fecha_publicacion_parcial, fecha_publicacion_valida,&#10;        idioma, idioma_valido, paginas, formato, categoria, precio, moneda, moneda_valida&#10;    Además, se añaden campos de trazabilidad: canonical_key, book_id, fuente_ganadora,&#10;    provenance (JSON por campo) y ts_ultima_actualizacion.&#10;    &quot;&quot;&quot;&#10;    if not isinstance(merged, pd.DataFrame):&#10;        return pd.DataFrame()&#10;    df = merged.copy()&#10;&#10;    # Normalizar isbn13 y flags si están disponibles&#10;    if 'isbn13' in df.columns:&#10;        try:&#10;            df['isbn13_norm'] = df['isbn13'].apply(lambda v: try_normalize_isbn(v)[1] if pd.notna(v) and str(v).strip() != '' else None)&#10;            df['isbn13_valido'] = df['isbn13_norm'].apply(lambda v: bool(is_valid_isbn13(str(v))) if pd.notna(v) and str(v).strip() != '' else False)&#10;        except Exception:&#10;            df['isbn13_norm'] = df['isbn13']&#10;            df['isbn13_valido'] = False&#10;    else:&#10;        df['isbn13_norm'] = None&#10;        df['isbn13_valido'] = False&#10;&#10;    # Función para obtener la clave canonical (prefiere isbn13 válido)&#10;    def _make_key(row):&#10;        try:&#10;            if row.get('isbn13_valido'):&#10;                return str(row.get('isbn13_norm'))&#10;        except Exception:&#10;            pass&#10;        return _canonical_key(row.get('titulo'), row.get('autor_principal'), row.get('editorial'), row.get('anio_publicacion'))&#10;&#10;    df['canonical_key'] = df.apply(lambda r: _make_key(r), axis=1)&#10;&#10;    # Campos originales de Goodreads (mapeo esperado)&#10;    gr_fields = ['titulo', 'autor_principal', 'rating', 'ratings_count', 'book_url', 'isbn10', 'isbn13']&#10;    # Campos adicionales interesantes desde Google Books&#10;    gb_extra = ['editorial', 'fecha_publicacion', 'fecha_publicacion_parcial', 'fecha_publicacion_valida', 'idioma', 'idioma_valido', 'paginas', 'formato', 'categoria', 'precio', 'moneda', 'moneda_valida']&#10;&#10;    survivors = []&#10;    # Agrupar por canonical_key: producir un registro canónico por libro&#10;    for key, group in df.groupby('canonical_key', dropna=False):&#10;        try:&#10;            # Intentar ordenar el grupo para preferir filas con más información&#10;            try:&#10;                if 'gb_match_score' in group.columns:&#10;                    group['_gb_ms'] = pd.to_numeric(group['gb_match_score'], errors='coerce').fillna(0.0)&#10;                else:&#10;                    group['_gb_ms'] = 0.0&#10;            except Exception:&#10;                group['_gb_ms'] = 0.0&#10;&#10;            def _completeness(row):&#10;                score_keys = ['titulo', 'autor_principal', 'editorial', 'fecha_publicacion', 'autores', 'precio', 'moneda']&#10;                s = sum(1 for k in score_keys if pd.notna(row.get(k)) and str(row.get(k)).strip() != '')&#10;                if row.get('isbn13_valido'):&#10;                    s += 2&#10;                try:&#10;                    s += float(row.get('gb_match_score')) if row.get('gb_match_score') is not None and str(row.get('gb_match_score')).strip() != '' else 0.0&#10;                except Exception:&#10;                    pass&#10;                return float(s)&#10;&#10;            group['_completeness'] = group.apply(lambda r: _completeness(r), axis=1)&#10;            group_sorted = group.sort_values(by=['_completeness', '_gb_ms'], ascending=[False, False])&#10;            # Empezamos con la fila &quot;winner&quot; pero no asumimos que contiene todos los campos&#10;            winner = group_sorted.iloc[0].copy() if not group_sorted.empty else pd.Series()&#10;&#10;            # Construir registro canónico priorizando campos de Goodreads (_gr) y rellenando desde GB (_gb)&#10;            rec = {}&#10;            provenance = {}&#10;            # Primer, campos de Goodreads; preferir *_gr si existe, sino coalesce con base column y luego *_gb&#10;            for f in gr_fields:&#10;                val = None&#10;                src = None&#10;                gr_key = f + '_gr'&#10;                gb_key = f + '_gb'&#10;                # Comprobar variantes: columnas ya coalesced sin sufijo pueden existir&#10;                if gr_key in group.columns and pd.notna(group_sorted.iloc[0].get(gr_key)) and str(group_sorted.iloc[0].get(gr_key)).strip() != '':&#10;                    val = group_sorted.iloc[0].get(gr_key)&#10;                    src = 'goodreads'&#10;                else:&#10;                    # buscar cualquier fila del grupo que tenga *_gr&#10;                    found = None&#10;                    try:&#10;                        for _, r in group.iterrows():&#10;                            if gr_key in r.index and pd.notna(r.get(gr_key)) and str(r.get(gr_key)).strip() != '':&#10;                                found = r.get(gr_key)&#10;                                break&#10;                    except Exception:&#10;                        found = None&#10;                    if found is not None:&#10;                        val = found&#10;                        src = 'goodreads'&#10;                    else:&#10;                        # fallback a columna base (coalesced) si existe&#10;                        if f in group_sorted.columns and pd.notna(group_sorted.iloc[0].get(f)) and str(group_sorted.iloc[0].get(f)).strip() != '':&#10;                            val = group_sorted.iloc[0].get(f)&#10;                            st = group_sorted.iloc[0].get('_source_type')&#10;                            if isinstance(st, str) and st == 'gb_only':&#10;                                src = 'google_books'&#10;                            else:&#10;                                src = 'merged'&#10;                        else:&#10;                            # intentar llenar desde *_gb&#10;                            try:&#10;                                for _, r in group.iterrows():&#10;                                    if gb_key in r.index and pd.notna(r.get(gb_key)) and str(r.get(gb_key)).strip() != '':&#10;                                        val = r.get(gb_key)&#10;                                        src = 'google_books'&#10;                                        break&#10;                            except Exception:&#10;                                val = None&#10;                                src = None&#10;                rec[f] = val&#10;                provenance[f] = src&#10;&#10;            # Ahora campos adicionales desde Google Books (relleno o añadido)&#10;            for f in gb_extra:&#10;                val = None&#10;                src = None&#10;                # Si ya existe en winner with base name, usarlo; else intentar desde *_gb&#10;                if f in group_sorted.columns and pd.notna(group_sorted.iloc[0].get(f)) and str(group_sorted.iloc[0].get(f)).strip() != '':&#10;                    val = group_sorted.iloc[0].get(f)&#10;                    st = group_sorted.iloc[0].get('_source_type')&#10;                    if isinstance(st, str) and st == 'gb_only':&#10;                        src = 'google_books'&#10;                    else:&#10;                        src = 'merged'&#10;                else:&#10;                    gb_key = f + '_gb'&#10;                    try:&#10;                        for _, r in group.iterrows():&#10;                            if gb_key in r.index and pd.notna(r.get(gb_key)) and str(r.get(gb_key)).strip() != '':&#10;                                val = r.get(gb_key)&#10;                                src = 'google_books'&#10;                                break&#10;                    except Exception:&#10;                        val = None&#10;                        src = None&#10;                # Guardar valor y procedencia&#10;                rec[f] = val&#10;                provenance[f] = src&#10;&#10;            # Normalizaciones y flags: aprovechar funciones existentes&#10;            # fecha_publicacion -&gt; ISO + parcial flag&#10;            try:&#10;                if rec.get('fecha_publicacion') is not None and str(rec.get('fecha_publicacion')).strip() != '':&#10;                    iso, parcial = parse_date_to_iso(rec.get('fecha_publicacion'))&#10;                    rec['fecha_publicacion'] = iso&#10;                    rec['fecha_publicacion_parcial'] = bool(parcial)&#10;                    rec['fecha_publicacion_valida'] = bool(iso is not None)&#10;                else:&#10;                    rec['fecha_publicacion'] = None&#10;                    rec['fecha_publicacion_parcial'] = False&#10;                    rec['fecha_publicacion_valida'] = False&#10;            except Exception:&#10;                rec['fecha_publicacion_parcial'] = False&#10;                rec['fecha_publicacion_valida'] = False&#10;&#10;            # idioma&#10;            try:&#10;                if rec.get('idioma') is not None and str(rec.get('idioma')).strip() != '':&#10;                    nl = normalize_language(rec.get('idioma'))&#10;                    ok = validate_language(nl)&#10;                    rec['idioma'] = nl&#10;                    rec['idioma_valido'] = bool(ok)&#10;                else:&#10;                    rec['idioma'] = None&#10;                    rec['idioma_valido'] = False&#10;            except Exception:&#10;                rec['idioma_valido'] = False&#10;&#10;            # moneda valid&#10;            try:&#10;                if rec.get('moneda') is not None and str(rec.get('moneda')).strip() != '':&#10;                    nc = normalize_currency(rec.get('moneda'))&#10;                    ok = validate_currency(nc)&#10;                    rec['moneda'] = nc&#10;                    rec['moneda_valida'] = bool(ok)&#10;                else:&#10;                    rec['moneda'] = None&#10;                    rec['moneda_valida'] = False&#10;            except Exception:&#10;                rec['moneda_valida'] = False&#10;&#10;            # precio coercion&#10;            try:&#10;                if rec.get('precio') is None or (isinstance(rec.get('precio'), float) and pd.isna(rec.get('precio'))) or (isinstance(rec.get('precio'), str) and str(rec.get('precio')).strip() == ''):&#10;                    rec['precio'] = None&#10;                else:&#10;                    s = str(rec.get('precio')).strip().replace(',', '.')&#10;                    s2 = ''.join([c for c in s if (c.isdigit() or c in '.-')])&#10;                    rec['precio'] = float(s2) if s2 not in ('', '.', '-') else None&#10;            except Exception:&#10;                rec['precio'] = None&#10;&#10;            # autores y categoria: unir listas desduplicando (usar helper existente)&#10;            try:&#10;                merged_autores = None&#10;                merged_categoria = None&#10;                for _, r in group.iterrows():&#10;                    # prefer *_gr authors first&#10;                    a_gr = r.get('autores_gr') if 'autores_gr' in r.index else None&#10;                    a_gb = r.get('autores_gb') if 'autores_gb' in r.index else None&#10;                    merged_autores = _merge_list_fields(merged_autores, a_gr)&#10;                    merged_autores = _merge_list_fields(merged_autores, a_gb)&#10;                    merged_categoria = _merge_list_fields(merged_categoria, r.get('categoria_gr') if 'categoria_gr' in r.index else None)&#10;                    merged_categoria = _merge_list_fields(merged_categoria, r.get('categoria_gb') if 'categoria_gb' in r.index else None)&#10;                rec['autores'] = merged_autores&#10;                rec['categoria'] = merged_categoria&#10;            except Exception:&#10;                rec['autores'] = rec.get('autores')&#10;                rec['categoria'] = rec.get('categoria')&#10;&#10;            # Determinar fuente_ganadora: preferir la fila con mayor completitud (ya ordenado)&#10;            try:&#10;                # el winner row_sorted[0] puede indicar la fuente predominante&#10;                fg = None&#10;                if 'source_name' in winner.index and pd.notna(winner.get('source_name')):&#10;                    fg = winner.get('source_name')&#10;                else:&#10;                    # Inferir a partir de provenance counts&#10;                    counts = {'goodreads': 0, 'google_books': 0}&#10;                    for v in provenance.values():&#10;                        if v == 'goodreads':&#10;                            counts['goodreads'] += 1&#10;                        elif v == 'google_books':&#10;                            counts['google_books'] += 1&#10;                    if counts['google_books'] &gt; counts['goodreads']:&#10;                        fg = 'google_books'&#10;                    elif counts['goodreads'] &gt;= counts['google_books']:&#10;                        fg = 'goodreads'&#10;                    else:&#10;                        fg = 'merged'&#10;                rec['fuente_ganadora'] = fg&#10;            except Exception:&#10;                rec['fuente_ganadora'] = winner.get('source_name') if 'source_name' in winner.index else None&#10;&#10;            # Book id: preferir isbn13 válido, sino canonical_key&#10;            try:&#10;                if rec.get('isbn13') is not None and str(rec.get('isbn13')).strip() != '' and is_valid_isbn13(str(try_normalize_isbn(rec.get('isbn13'))[1])):&#10;                    rec['book_id'] = try_normalize_isbn(rec.get('isbn13'))[1]&#10;                    rec['book_id_method'] = 'isbn13'&#10;                else:&#10;                    rec['book_id'] = key&#10;                    rec['book_id_method'] = 'fallback_sha1'&#10;            except Exception:&#10;                rec['book_id'] = key&#10;                rec['book_id_method'] = 'fallback_sha1'&#10;&#10;            # ts_ultima_actualizacion: mayor ts_ingest del grupo si existe, sino ahora&#10;            try:&#10;                if 'ts_ingest' in group.columns:&#10;                    ts_vals = [r for r in group.get('ts_ingest').tolist() if r is not None]&#10;                    if ts_vals:&#10;                        # tomar max lexicográfico/ISO&#10;                        rec['ts_ultima_actualizacion'] = max(ts_vals)&#10;                    else:&#10;                        rec['ts_ultima_actualizacion'] = datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;                else:&#10;                    rec['ts_ultima_actualizacion'] = datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;            except Exception:&#10;                rec['ts_ultima_actualizacion'] = datetime.now(timezone.utc).isoformat(timespec='seconds')&#10;&#10;            # Guardar canonical_key&#10;            rec['canonical_key'] = key&#10;&#10;            # Adjuntar provenance JSON&#10;            try:&#10;                rec['provenance'] = json.dumps(provenance, ensure_ascii=False)&#10;            except Exception:&#10;                rec['provenance'] = json.dumps({k: provenance.get(k) for k in provenance}, ensure_ascii=False)&#10;&#10;            survivors.append(rec)&#10;        except Exception:&#10;            # no bloquear el bucle de agrupación&#10;            continue&#10;&#10;    if survivors:&#10;        dim = pd.DataFrame(survivors)&#10;    else:&#10;        dim = pd.DataFrame()&#10;&#10;    # Asegurar columnas esperadas y orden&#10;    desired = ['canonical_key','book_id','book_id_method'] + gr_fields + gb_extra + ['fuente_ganadora','provenance','ts_ultima_actualizacion']&#10;    for d in desired:&#10;        if d not in dim.columns:&#10;            dim[d] = None&#10;    cols_ordered = [c for c in desired if c in dim.columns] + [c for c in dim.columns if c not in desired]&#10;    try:&#10;        dim = dim[cols_ordered]&#10;    except Exception:&#10;        pass&#10;    return dim&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>